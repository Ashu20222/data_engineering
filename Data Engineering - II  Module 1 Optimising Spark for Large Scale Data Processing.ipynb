{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fedff6",
   "metadata": {},
   "source": [
    "# Optimising Disk IO for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15483a",
   "metadata": {},
   "source": [
    "## Spinning Up a Spark EMR Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d61a5",
   "metadata": {},
   "source": [
    "Amazon EMR\n",
    "Which of the following is not a benefit of Amazon EMR cluster?\n",
    "\n",
    "\n",
    "Ease of use\n",
    "\n",
    "\n",
    "Flexible and reliable\n",
    "\n",
    "\n",
    "Secure\n",
    "\n",
    "\n",
    "High cost   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd3143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1e533f8",
   "metadata": {},
   "source": [
    "Amazon EMR\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "“Amazon EMR allows a temporary shut down of the clusters.”\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7164dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ecd414",
   "metadata": {},
   "source": [
    "Amazon EMR\n",
    "For which of the following types of nodes does EMR ensure high availability?\n",
    "\n",
    "\n",
    "Master node\n",
    "\n",
    "Slave nodes  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd5450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37fb1894",
   "metadata": {},
   "source": [
    "## Analysing a Spark job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7beba",
   "metadata": {},
   "source": [
    "Components of a Spark Job\n",
    "Which of the following is considered as the smallest unit of work in Spark terminology?\n",
    "\n",
    "\n",
    "Application\n",
    "\n",
    "\n",
    "Job\n",
    "\n",
    "\n",
    "Task   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558abc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54b224f2",
   "metadata": {},
   "source": [
    "Spark Execution\n",
    "In Spark, the actual execution of the program only takes place when an Action is called. True or false?\n",
    "\n",
    "\n",
    "True  ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e6258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4736c7e6",
   "metadata": {},
   "source": [
    "Spark Execution\n",
    "You can see the details of each task and stage in a Spark job using the Spark History server. True or false?\n",
    "\n",
    "\n",
    "True  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df734f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7cad12b",
   "metadata": {},
   "source": [
    "## Why Optimise a Spark job?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44cc2b",
   "metadata": {},
   "source": [
    "Spark Optimisation\n",
    "Which of the following are Key Performance Metrics that must be taken into consideration while optimising your Spark job?\n",
    "\n",
    "\n",
    "Stages and Tasks   ✓ Correct\n",
    "\n",
    "Spark Environment Information   ✓ Correct\n",
    "\n",
    "Information about Executors   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3342381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29fbdeb6",
   "metadata": {},
   "source": [
    "Spark Optimisation\n",
    "Spark jobs can be optimised both from the perspective of reducing execution time and improving resource utilisation. Which of these perspectives should you prioritise when optimising Spark jobs?\n",
    "\n",
    "\n",
    "Prioritise reduction of execution time\n",
    "\n",
    "\n",
    "Prioritise maintaining resource utilisation\n",
    "\n",
    "Optimise jobs taking into account both the perspectives in a balanced manner   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d7904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdef001e",
   "metadata": {},
   "source": [
    "Spark Optimisation\n",
    "Which of the following methods of optimisation will reduce the Disk IO and which ones will reduce the Network IO?\n",
    "\n",
    "Proper Serialisation techniques\n",
    "\n",
    "Storing data in optimised file formats\n",
    "\n",
    "Partitioning\n",
    "\n",
    "Optimising Joins\n",
    "\n",
    "Using Caching strategy\n",
    "\n",
    "\n",
    "Disk IO - 1,2,5 Network IO - 3,4  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780a094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c92bd4",
   "metadata": {},
   "source": [
    "## Understanding Disk IO in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8f659",
   "metadata": {},
   "source": [
    "Disk IO\n",
    "Which of the following types of delays does not correspond to total delay in the process of Disk IO?\n",
    "\n",
    "\n",
    "Seek Delay\n",
    "\n",
    "\n",
    "Rotational Delay of HDD\n",
    "\n",
    "\n",
    "Transfer Delay\n",
    "\n",
    "\n",
    "None of the above  ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74755913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a43cfcee",
   "metadata": {},
   "source": [
    "Disk IO\n",
    "Which of the following is not a method for reducing Disk IO?\n",
    "\n",
    "\n",
    "Using columnar file formats\n",
    "\n",
    "\n",
    "Using Cache strategies\n",
    "\n",
    "\n",
    "Using Custom Partitioner techniques   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae4715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18771acc",
   "metadata": {},
   "source": [
    "## Using Various File Formats in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc7fca",
   "metadata": {},
   "source": [
    "File Formats in Spark\n",
    "Read this (https://blog.clairvoyantsoft.com/big-data-file-formats-3fb659903271) article and answer the following question:\n",
    "\n",
    "Suppose you have to store your Spark DataFrames locally, but you have strict storage constraints. Which of the following file formats would be the best choice for this use case?\n",
    "\n",
    "\n",
    "Parquet\n",
    "\n",
    "\n",
    "Avro\n",
    "\n",
    "\n",
    "ORC   ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da62a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ded849e",
   "metadata": {},
   "source": [
    "File Formats in Spark\n",
    "Which of the following is a benefit of using columnar file formats?\n",
    "\n",
    "\n",
    "Better compression\n",
    "\n",
    "\n",
    "Reduced Disk IO\n",
    "\n",
    "\n",
    "Better support for modern processors\n",
    "\n",
    "\n",
    "All of the above  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25302477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753a815b",
   "metadata": {},
   "source": [
    "## Serialization and Deserialization in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbe647",
   "metadata": {},
   "source": [
    "Serialization in PySpark\n",
    "State whether the following is true or false.\n",
    "\n",
    "“Serialization is only helpful when you want to save objects to a disk and not when you want to send them over networks”.\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bec8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc54125c",
   "metadata": {},
   "source": [
    "Serialization in PySpark\n",
    "Let’s assume that you have to create a Spark application; the data comes from various sources and you have to use multiple different types of Python objects for their processing. Which serializer will you prefer in this case?\n",
    "\n",
    "\n",
    "Marshal Serializer\n",
    "\n",
    "\n",
    "Pickle Serializer  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341e295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ee46b0",
   "metadata": {},
   "source": [
    "## Spark Memory Management Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb110ce9",
   "metadata": {},
   "source": [
    "Spark Memory Management Parameters\n",
    "Which of the following is the default Memory level supported for RDDs with cache()?\n",
    "\n",
    "\n",
    "MEMORY_ONLY   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba08995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6facb8aa",
   "metadata": {},
   "source": [
    "Spark Memory Management Parameters\n",
    "Which of the following is the default Memory level supported for Datasets with cache()?\n",
    "\n",
    "\n",
    "MEMORY_ONLY\n",
    "\n",
    "\n",
    "MEMORY_AND_DISK   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37017538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90857b51",
   "metadata": {},
   "source": [
    "## Practice Coding Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98c842",
   "metadata": {},
   "source": [
    "File Formats in Spark\n",
    "Load the data given above in CSV format, save it as a Parquet file and name it ‘sales_data.parquet’, Now, analyse the saved data and identify the difference in data size. While saving, set partition to 1 so that you can clearly view the data size. Which of the following is correct?\n",
    "\n",
    "\n",
    "The Parquet file is around 1/3 of the size of the CSV file.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f0184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5cf0d59",
   "metadata": {},
   "source": [
    "File Formats in Spark\n",
    "Now, we have two data sets:\n",
    "\n",
    "1: CSV file\n",
    "\n",
    "2. Parquet file\n",
    "\n",
    "Query: Load the data and perform the following query:\n",
    "\n",
    "Filter data for Region == Europe\n",
    "\n",
    "Group by Country \n",
    "\n",
    "Count the number of sales in offline and online transactions.\n",
    "\n",
    "Sales channel == ‘Offline’ - its offline transaction\n",
    "\n",
    "Sales channel == Online - its online transaction\n",
    "\n",
    "Sort the orders based on the number of offline transactions in descending order and get the top three orders out of it.\n",
    "\n",
    "Perform the aforementioned query by loading the CSV file and Parquet file one by one and calculate the time difference between them. Also, state in which data set the operation was faster and by approximately how much.\n",
    "\n",
    "\n",
    "Operation on the parquet data file is about 2–3 times faster than that on the CSV data file.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b39df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97752462",
   "metadata": {},
   "source": [
    "Caching in Spark\n",
    "Perform the following operations on the data set.\n",
    "\n",
    "Query: Load the data and perform the following query:\n",
    "\n",
    "Filter data for Region == Europe\n",
    "\n",
    "Group by Country\n",
    "\n",
    "Count the number of sales in offline and online transactions.\n",
    "\n",
    "Sales channel == ‘Offline’ - its offline transaction\n",
    "\n",
    "Sales channel == Online - its online transaction\n",
    "\n",
    "Cache the DataFrame here\n",
    "\n",
    "Sort the data frame in ascending order and get the top three.\n",
    "\n",
    "Sort the data frame in ascending order and get the top three.\n",
    "\n",
    "Now, run these steps in two flavours: with step 4 and without step 4.\n",
    "\n",
    "Which of the aforementioned variants of code, with caching and without caching, is faster, and by how much?\n",
    "\n",
    "\n",
    "“With caching” is about 100 times faster than without caching.\n",
    "\n",
    "\n",
    "“With caching” is about 15 times faster than without caching.\n",
    "\n",
    "\n",
    "“With caching” is about 2–4 times faster than without caching.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2999b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd5d435",
   "metadata": {},
   "source": [
    "# Optimising Network IO for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c65faa",
   "metadata": {},
   "source": [
    "## Understanding Network IO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49717b94",
   "metadata": {},
   "source": [
    "Network IO in Spark\n",
    "Which of the following are the methods to reduce Shuffles in Spark?\n",
    "\n",
    "\n",
    "Optimising Joins   ✓ Correct\n",
    "\n",
    "Avoiding Wide Transformations   ✓ Correct\n",
    "\n",
    "Using appropriate partitioning techniques   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55a0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ef3c86a",
   "metadata": {},
   "source": [
    "Network IO in Spark\n",
    "Data locality ensures that IO operations are done near the physical nodes. True or false?\n",
    "\n",
    "\n",
    "True  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288eaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82742e77",
   "metadata": {},
   "source": [
    "## Understanding Shuffles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5def52a",
   "metadata": {},
   "source": [
    "huffles in Spark\n",
    "Which of the following operations does not involve a shuffle process?\n",
    "\n",
    "\n",
    "cogroup\n",
    "\n",
    "\n",
    "join\n",
    "\n",
    "\n",
    "groupByKey\n",
    "\n",
    "\n",
    "collect  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00663580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2087874",
   "metadata": {},
   "source": [
    "Shuffles in Spark\n",
    "The groupByKey is better than the reduceByKey as it groups all the keys in one node before performing any operation. True or false?\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d846c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0d199be",
   "metadata": {},
   "source": [
    "## Optimising Joins in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9239f",
   "metadata": {},
   "source": [
    "Optimising Joins in Spark\n",
    "Joins is one of the most commonly used and heaviest in terms of compute load in an ETL application. True or false?\n",
    "\n",
    "\n",
    "True  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740237f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "287985f2",
   "metadata": {},
   "source": [
    "Optimising Joins in Spark\n",
    "Should Broadcast joins always be used instead of Shuffle Hash joins?\n",
    "\n",
    "\n",
    "No, it should be used only when there are small tables to be joined with bigger tables.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5ac16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f45d114",
   "metadata": {},
   "source": [
    "## Understanding Data Partitioning in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87220112",
   "metadata": {},
   "source": [
    "Data Partitioning in Spark\n",
    "The number of executors, cores and partitions directly affect the degree of parallelism in Spark Jobs. True or false?\n",
    "\n",
    "\n",
    "True  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af93ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69c4dbc5",
   "metadata": {},
   "source": [
    "Data Partitioning in Spark\n",
    "Which of the following operations help us in changing the number of partitions in Spark?\n",
    "\n",
    "\n",
    "Repartition  ✓ Correct\n",
    "\n",
    "Coalesce  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942c15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d573cc90",
   "metadata": {},
   "source": [
    "Data Partitioning in Spark\n",
    "Assume that in Partitions 1-4, the data is present as follows:\n",
    "\n",
    "Partition 1 - a, b, c, d\n",
    "\n",
    "Partition 2 - e, f\n",
    "\n",
    "Partition 3 - g, h, i, j\n",
    "\n",
    "Partition 4 - k, l\n",
    "\n",
    "Now, let’s say a coalesce(2) operation is run on these partitions. What would be the expected result?\n",
    "\n",
    "\n",
    "Partition 1 - a, b, c, d, g, h, i, j; Partition 2 - e, f, k, l \n",
    "\n",
    "\n",
    "Partition 1 - a, b, c, d, e, f; Partition 2 - g, h, i, j, k, l  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65914d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69a18429",
   "metadata": {},
   "source": [
    "Data Partitioning in Spark\n",
    "With Custom Partitioning, you can optimise the partitioning mechanisms for your data to ensure that similar data are in the same partitions. True or false?\n",
    "\n",
    "\n",
    "True ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99669911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb981760",
   "metadata": {},
   "source": [
    "Data Partitioning in Spark\n",
    "Why is it recommended that the number of partitions should be about two to three times the number of physical cores?\n",
    "\n",
    "\n",
    "The higher number of partitions ensures that the maximum degree of parallelism can be utilised from the available hardware resources.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552eaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0220fb81",
   "metadata": {},
   "source": [
    "## Practice Coding Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26b2e5",
   "metadata": {},
   "source": [
    "Optimising Joins in Spark\n",
    "Perform the following operations on the data set.\n",
    "\n",
    "Query: Load the data and perform the following query:\n",
    "\n",
    "Calculate the total sales for each country.\n",
    "\n",
    "Join the DataFrame given above with the actual DataFrame; use country as key.\n",
    "\n",
    "df.join(df_agg,df_agg.Country==df.Country) - Shuffle Hash join\n",
    "\n",
    "df.join(broadcast(df_agg),df_agg.Country==df.Country) - Broadcast join\n",
    "\n",
    "Try joining them directly and with the help of broadcast join as well.\n",
    "\n",
    "Identify the execution speed of both the operations separately.\n",
    "\n",
    "Which of these joins is faster?\n",
    "\n",
    "\n",
    "Broadcast join  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09241f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05a660b1",
   "metadata": {},
   "source": [
    "Data Partitioning in Spark\n",
    "Load the CSV data set and perform the following operations:\n",
    "\n",
    "Repartition it into 10 partitions and save the data as CSV files.\n",
    "\n",
    "Coalesce it into 10 partitions and save the data as CSV files.\n",
    "\n",
    "Note: You can save these files into S3 to check the size of the CSV files.\n",
    "\n",
    "State whether the following statements are true or false.\n",
    "\n",
    "All 10 partitions in the case of repartition are of the same size.\n",
    "\n",
    "All 10 partitions in the case of Coalesce are of the same size.\n",
    "\n",
    "\n",
    "True; True\n",
    "\n",
    "\n",
    "True; False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600c9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e53c50ba",
   "metadata": {},
   "source": [
    "# Optimising the Spark Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f966e",
   "metadata": {},
   "source": [
    "## Why Optimise Cluster Utilisation for Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738ea05",
   "metadata": {},
   "source": [
    "Optimising Spark Clusters\n",
    "Which of the following are important parameters that one must keep in mind while optimising cluster utilisation?\n",
    "\n",
    "\n",
    "Driver Memory   ✓ Correct\n",
    "\n",
    "Executor Memory   ✓ Correct\n",
    "\n",
    "Executor Cores   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5ae87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6222378e",
   "metadata": {},
   "source": [
    "Optimising Spark Clusters\n",
    "You can decide the size of Driver and Executor memory based on the size of the job. True or false?\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e25bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81cb9570",
   "metadata": {},
   "source": [
    "## Job Deployment Modes in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2aef8",
   "metadata": {},
   "source": [
    "Spark Deployment Modes\n",
    "In which of the following use cases should Local Mode be used in Spark?\n",
    "\n",
    "\n",
    "For testing, debugging and demonstration purposes   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c41d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aaf2668",
   "metadata": {},
   "source": [
    "Spark Deployment Modes\n",
    "What is the difference between Spark Cluster mode and Spark Client mode?\n",
    "\n",
    "\n",
    "In the Cluster mode, the job is executed in the cluster nodes, while in the Client mode, the job is executed on the Client machine.\n",
    "\n",
    "\n",
    "In the Cluster mode, the Driver program resides in one of the worker nodes inside the cluster, while in the Client mode, the Driver program resides in the external Client machine.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523be7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19cb4211",
   "metadata": {},
   "source": [
    "## Tuning Spark Memory and CPU Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ca4f4",
   "metadata": {},
   "source": [
    "Tuning Spark Cluster parameters\n",
    "Which of the following parameters can be used to set the number of executor cores?\n",
    "\n",
    "\n",
    "spark.executor.parallelism\n",
    "\n",
    "\n",
    "spark.executor.cores   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8c914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23908af1",
   "metadata": {},
   "source": [
    "Tuning Spark Cluster parameters\n",
    "Is it necessary to always set up the number of executor instances for a Spark job?\n",
    "\n",
    "\n",
    "Yes, you need to define it every time before starting the Spark job.\n",
    "\n",
    "\n",
    "No, you do not have to define it every time before starting the Spark job.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21935684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0afd91f3",
   "metadata": {},
   "source": [
    "## Apache Spark in the Production Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00ffcb",
   "metadata": {},
   "source": [
    "Spark in Production Environment\n",
    "In the industry environment, which of the following practices are usually followed for Spark job deployment?\n",
    "\n",
    "\n",
    "With each successful build, a complete Spark job is packaged  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a14fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e5c852e",
   "metadata": {},
   "source": [
    "## Best Practices While Working with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30dcf2",
   "metadata": {},
   "source": [
    "Tuning Spark Cluster Parameters\n",
    "Starting from the default configuration and then monitoring and tweaking the configurations is the ideal way to optimise your Spark job. True or false?\n",
    "\n",
    "\n",
    "True  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bb091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ed7a6e",
   "metadata": {},
   "source": [
    "Tuning Spark Cluster Parameters\n",
    "Which of the following is not a correct Common error and solution Approach pair?\n",
    "\n",
    "\n",
    "Driver OOM - Increase the Driver Memory\n",
    "\n",
    "\n",
    "Executor OOM - Increase the Executor Memory\n",
    "\n",
    "\n",
    "Too big Shuffle block - Keep the tasks small\n",
    "\n",
    "\n",
    "Frequent Garbage Collection - Increase the size of the tasks  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52b4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de62f038",
   "metadata": {},
   "source": [
    "# Graded Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a6323",
   "metadata": {},
   "source": [
    "Q1 File Formats in Spark\n",
    "Which of the following are advantages of Parquet file format?\n",
    "\n",
    "Note: More than one option can be correct. \n",
    "\n",
    "\n",
    "Parquet file format is columnar; so, it reduces disk space.  ✓ Correct\n",
    "\n",
    "IO operations are reduced, as we scan only a subset of the data.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011be4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5f49b7",
   "metadata": {},
   "source": [
    "Q2 Spark Deployment Modes\n",
    "State whether the following statement is true or false. \n",
    "'Spark cannot run without HDFS'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd444d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99bed688",
   "metadata": {},
   "source": [
    "Q3 Tuning Spark Cluster parameters\n",
    "How can we define the executor memory for a Spark program? Select all the correct options from below.\n",
    "\n",
    "Note: More than one option can be correct. \n",
    "\n",
    "\n",
    "By passing the configuration during spark-submit command   ✓ Correct\n",
    "\n",
    "By passing the configuration during Spark context initialisation   ✓ Correct\n",
    "\n",
    "By setting Spark configuration in spark-defaults.conf   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa0094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a1edce",
   "metadata": {},
   "source": [
    "Q4 Spark Deployment Modes\n",
    "In the case of spark-submit in cluster mode on the cluster, can Spark driver and Spark executor run on the same machine/node?\n",
    "\n",
    "\n",
    "Yes  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22418c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2299ced",
   "metadata": {},
   "source": [
    "Q5 Spark Memory Management Parameters\n",
    "In terms of caching/persist and checkpoints, which of the following statement/s is/are valid?\n",
    "\n",
    "\n",
    "Persists can store the RDD in Memory and Disk.\n",
    "\n",
    "\n",
    "Checkpoint can store the RDD in Disk only.\n",
    "\n",
    "\n",
    "Persist will keep the lineage of the RDD.\n",
    "\n",
    "\n",
    "All of the above   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef1323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078e4d12",
   "metadata": {},
   "source": [
    "Q6 Data Partitioning in Spark\n",
    "Write an appropriate one-line code for the following problem. \n",
    "\n",
    "You have a Spark data frame that has 100 partitions. You want to reduce the partition to 50 with less data shuffling.\n",
    "\n",
    "df = spark.read.option(\"header\",\"true\").csv(data.csv')\n",
    "#Write your code here to reduce the partition to 50 with less data shuffling \n",
    " \n",
    "\n",
    "\n",
    "df2 = df.partition(50)\n",
    "\n",
    "df2 = df.repartition(50)\n",
    "\n",
    "df2 = df.coalesce(50)  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b8823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0931af2",
   "metadata": {},
   "source": [
    "Q7 Shuffles in Spark\n",
    "Which of the following Spark reduce operations has the highest network data shuffling?\n",
    "\n",
    "\n",
    "reduceByKey\n",
    "\n",
    "\n",
    "AggregateByKey\n",
    "\n",
    "\n",
    "combineByKey\n",
    "\n",
    "\n",
    "groupByKey  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69087656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad12dad",
   "metadata": {},
   "source": [
    "Q8 Optimising Joins in Spark\n",
    "The following code is using shuffleHashJoin for joining two DataFrames. \n",
    "\n",
    "Select the correct option to convert shuffleHashJoin to BroadcastHashJoin at line 3.\n",
    "\n",
    "df = spark.read.option(\"header\",\"true\").csv(data.csv')\n",
    "df_agg = df.groupby(\"Department\").agg(F.count(\"studentsl\").alias('total_students'))\n",
    "#Select appropriate option for below line to convert the shuffle Hash Join to Broadcast Join\n",
    "final_df = df.join(df_agg,df_agg.Department==df.Department)\n",
    "final_df.show(3)\n",
    "final_df.explain(True)\n",
    " \n",
    "\n",
    "\n",
    "final_df = df.join(hashjoin(df_agg)_,df_agg.Department==df.Department)\n",
    "\n",
    "final_df = df.join(broadcast(df_agg)_,df_agg.Department==df.Department)   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ea105a9",
   "metadata": {},
   "source": [
    "Q9 Spark Cluster Configuration\n",
    "For the following Spark command, how many parallel tasks, at most, can execute?\n",
    "\n",
    "spark-submit --master yarn --deploy-mode client --num-executors 7 --executors-cores 3 mnistOnSpark.py\n",
    " \n",
    "\n",
    "\n",
    "7\n",
    "\n",
    "\n",
    "10\n",
    "\n",
    "\n",
    "21   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cacac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ae45736",
   "metadata": {},
   "source": [
    "Q10 Spark Architecture\n",
    "Which of the following Spark RDD/DF actions may need the Driver memory ‘spark.driver.memory’ to be increased?\n",
    "\n",
    "\n",
    "rdd.count()\n",
    "\n",
    "\n",
    "rdd.write()\n",
    "\n",
    "\n",
    "rdd.collect()   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e23430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb5910c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Committed successfully! https://jovian.ai/ashutoshgole18/data-engineering-ii-module-1-optimising-spark-for-large-scale-data-processing\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ashutoshgole18/data-engineering-ii-module-1-optimising-spark-for-large-scale-data-processing'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b58890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
