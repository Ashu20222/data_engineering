{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2803e6d7",
   "metadata": {},
   "source": [
    "# Introduction to Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284c501",
   "metadata": {},
   "source": [
    "## Introduction to Distributed Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee71c4",
   "metadata": {},
   "source": [
    "Distributed Systems\n",
    "Which of the following statements is correct?\n",
    " \n",
    "\n",
    "\n",
    "Distributed systems can take multiple requests at the same time.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a4b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd736703",
   "metadata": {},
   "source": [
    "Distributed Systems\n",
    "Suppose a local supermarket needs a data management system. What type of file system implementation would suffice their needs?\n",
    "\n",
    "\n",
    "A horizontally scalable cluster system with multiple commodity machines.\n",
    "\n",
    "\n",
    "A central-local server architecture for ease of managing a central node.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bfd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2b5a2b1",
   "metadata": {},
   "source": [
    "Distributed Systems\n",
    "How are distributed systems able to maintain data availability?\n",
    " \n",
    "\n",
    "\n",
    "Data is stored on a reliable central node.\n",
    "\n",
    "\n",
    "Data is stored and replicated over many nodes for maintaining fault tolerance.     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b054a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d6d79b9",
   "metadata": {},
   "source": [
    "## Introduction to GFS and MapReduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e493d16",
   "metadata": {},
   "source": [
    "Google File System\n",
    "Which of the following statements is correct?\n",
    "\n",
    "\n",
    "GFS comprised specialised enterprise-grade servers for the slave nodes.\n",
    "\n",
    "\n",
    "GFS can be easily scaled up by upgrading the existing servers with more RAM and disk space.\n",
    "\n",
    "\n",
    "If one server in the GFS system goes down, the whole system can still continue operating without any interruption to data availability.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d5b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fc80428",
   "metadata": {},
   "source": [
    "MapReduce\n",
    "Which of the following statements is correct?\n",
    "\n",
    "\n",
    "Computations done using MapReduce are faster in nature than computations using other programming models.\n",
    "\n",
    "\n",
    "MapReduce can work on both unstructured and structured data sets.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03cc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "016036fc",
   "metadata": {},
   "source": [
    "## Introduction to Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6519c",
   "metadata": {},
   "source": [
    "MapReduce\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"In MapReduce tasks, some slots are reserved for the Map phase and some for the Reduce phase.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537af0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "242217f3",
   "metadata": {},
   "source": [
    "HDFS\n",
    "Which of the following is the Secondary NameNode not responsible for?\n",
    "\n",
    "\n",
    "Keeping a log of all file edits\n",
    "\n",
    "\n",
    "Acting as a checkpoint for the NameNode\n",
    "\n",
    "\n",
    "Acting as the NameNode in case of active NameNode failure    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c01e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4072847",
   "metadata": {},
   "source": [
    "Hadoop\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"In a Hadoop cluster, a few nodes are used for storage and run HDFS only and the remaining nodes are used for processing and these run MapReduce only.\"\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b02e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d473e9a8",
   "metadata": {},
   "source": [
    "## Hadoop 2.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b99e7",
   "metadata": {},
   "source": [
    "Hadoop 2.x\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"Because of programmable slots, Hadoop 2.x is not limited to only the MapReduce programming model\"\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f7ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a37e784",
   "metadata": {},
   "source": [
    "Hadoop 2.x\n",
    "With Hadoop 2.x, a high number of jobs could be monitored better, because:\n",
    "\n",
    "\n",
    "More RAM was provided to the Master Node.\n",
    "\n",
    "\n",
    "YARN distributes the work of the Job Tracker to the Application Masters and the Resource Manager.     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17cb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cfcefc8",
   "metadata": {},
   "source": [
    "Improvements in Hadoop 3.x\n",
    "Which of the following are not advantages or improvements of Hadoop 3.x over Hadoop 2.x?\n",
    "\n",
    "\n",
    "GPU support\n",
    "\n",
    "\n",
    "Erasure coding support\n",
    "\n",
    "\n",
    "Support for other frameworks like Spark, Tez, etc.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1595be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51655044",
   "metadata": {},
   "source": [
    "## YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ac159",
   "metadata": {},
   "source": [
    "Hadoop 2.x\n",
    "Read the article(Introduction section) and answer the question that follows: ResourceManagerHA\n",
    "\n",
    "How does Hadoop 2.x solve the problem of a Single Point of Failure (SPOF) at the ResourceManager after Hadoop 2.4?\n",
    "\n",
    "\n",
    "It maintains a Standby Resource Manager Server.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6ebd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcc81edf",
   "metadata": {},
   "source": [
    "YARN\n",
    "Which of the following statements are true with respect to an Application Master? (Note: Multiple options may be correct.)\n",
    "\n",
    "\n",
    "It is responsible for monitoring the jobs of the slave nodes.     ✓ Correct\n",
    "\n",
    "Application Masters are present in the slave nodes for each individual job and are separate from the Resource Manager.     ✓ Correct\n",
    "\n",
    "It coordinates with the Node Manager for maintaining jobs in the Data Nodes.     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3feb0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b87652e",
   "metadata": {},
   "source": [
    "YARN\n",
    "Which aspect of YARN has helped in enhancing the scalability, flexibility and fault tolerance of Hadoop? (Note: Multiple options may be correct.)\n",
    "\n",
    "\n",
    "Having a dedicated Application Master for each job.    ✓ Correct\n",
    "\n",
    "Decoupling the resource management and scheduling components of Hadoop.    ✓ Correct\n",
    "\n",
    "Having multiple containers in the same Data Node.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3afac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b243a2b",
   "metadata": {},
   "source": [
    "## Task Processing in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6df0e5",
   "metadata": {},
   "source": [
    "Task Processing in Hadoop\n",
    "What are the responsibilities of the Resource Manager in task processing in Hadoop? (Note: Multiple options may be correct.)\n",
    "\n",
    "\n",
    "Managing cluster’s resources   ✓ Correct\n",
    "\n",
    "Checking on the Application Master’s status   ✓ Correct\n",
    "\n",
    "Checking on the health of each individual node and container\n",
    "\n",
    "\n",
    "Receiving jobs from the clients   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709fbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d556e68",
   "metadata": {},
   "source": [
    "Task Processing in Hadoop\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The Resource Manager assumes that a node is dead if it does not receive a heartbeat signal in the designated period of time.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25029b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3e6dbf1",
   "metadata": {},
   "source": [
    "## Tools for Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7c6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c3cc5f",
   "metadata": {},
   "source": [
    "Tools for Hadoop\n",
    "Which of the following tools provides a command-line interface for transferring data between relational databases and Hadoop?\n",
    "\n",
    "\n",
    "HBase\n",
    "\n",
    "\n",
    "Hive\n",
    "\n",
    "\n",
    "Sqoop   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded785cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45867956",
   "metadata": {},
   "source": [
    "ools for Hadoop\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"Oozie is a tool that allows automating the data pipelines.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3521891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "918ee1a6",
   "metadata": {},
   "source": [
    "Tools for Hadoop\n",
    "What are the differences between MapReduce and Spark? (Note: Multiple options may be correct.)\n",
    "\n",
    "\n",
    "Spark is a lot faster than MapReduce.     ✓ Correct\n",
    "\n",
    "Spark is harder to code than MapReduce, and it requires installing hardware.\n",
    "\n",
    "\n",
    "Spark provides low-latency computing, whereas MapReduce provides high-latency computing.     ✓ Correct\n",
    "\n",
    "Spark can also process real-time data, unlike MapReduce, which only supports batch processing.      ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c1916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eed7a947",
   "metadata": {},
   "source": [
    "# Introduction to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb800a6",
   "metadata": {},
   "source": [
    "## File Storage in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309a96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8261e92a",
   "metadata": {},
   "source": [
    "File Storage in HDFS\n",
    "Which of the following is incorrect about block size in Hadoop?\n",
    "\n",
    "\n",
    "The block size is 128 MB, as it helps in reducing metadata storage.\n",
    "\n",
    "\n",
    "The block size is 128 MB because the internet connection is sufficient for this block size.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353270f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d56e712",
   "metadata": {},
   "source": [
    "Distributed Systems\n",
    "Which of the following are features of horizontal scaling? (Note: Multiple options may be correct.)\n",
    "\n",
    "\n",
    "Can be done on the fly in Distributed Systems.    ✓ Correct\n",
    "\n",
    "Has no system downtime in Distributed Systems.    ✓ Correct\n",
    "\n",
    "Easily adds to the existing storage by adding new machines.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81a66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43d343fe",
   "metadata": {},
   "source": [
    "Hadoop\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"Hadoop is preferred to RDBMSs for real-time storage and querying critical data such as financial transactions.\"\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef6750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4251b7e",
   "metadata": {},
   "source": [
    "HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"HDFS is a distributed storage system for structured data only.\"\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b26bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c0fda7f",
   "metadata": {},
   "source": [
    "Hadoop\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"Unstructured and semi-structured data cannot be stored in an RDBMS at all.\"\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c876705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a253624",
   "metadata": {},
   "source": [
    "## Basic Commands in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749334d0",
   "metadata": {},
   "source": [
    "Basic Commands in HDFS\n",
    "Which command is used to transfer a file from the local file system[of the EMR instance] to HDFS?\n",
    "\n",
    "\n",
    "cp\n",
    "\n",
    "\n",
    "put   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936169c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a6f73c",
   "metadata": {},
   "source": [
    "Basic Commands in HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"We need to change the owner of the directory in HDFS to which a file is being sent, to the user who will be sending the file\"\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dafcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01bbfb0",
   "metadata": {},
   "source": [
    "Basic Commands in HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The replication factor cannot be changed in HDFS.\" \n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba57458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e19c81a",
   "metadata": {},
   "source": [
    "## Write Operation in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2aa2c",
   "metadata": {},
   "source": [
    "Write Operation in HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The lease given by the NameNode is important, as, without it, the client cannot write to HDFS.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a7e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7faeac72",
   "metadata": {},
   "source": [
    "Write Operation in HDFS\n",
    "Which of the following messages is not a part of the Write operation in HDFS?\n",
    "\n",
    "\n",
    "Write Complete\n",
    "\n",
    "\n",
    "Ack\n",
    "\n",
    "\n",
    "Create\n",
    "\n",
    "\n",
    "None of the above     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f851814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b840fd1",
   "metadata": {},
   "source": [
    "## Rack Awareness in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f3ae2c",
   "metadata": {},
   "source": [
    "Rack Awareness in Hadoop\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"It is not recommended to store all the replicas in different data centres even though this maximises the probability that at least one replica is available at all times.\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb646fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c142e1f",
   "metadata": {},
   "source": [
    "## Read Operation in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e1bb4",
   "metadata": {},
   "source": [
    "Read Operation in HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"HDFS client first verifies the checksum before sending the received packets to the client.\"\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881938f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b3876e",
   "metadata": {},
   "source": [
    "## Features and Limitations of HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377f127",
   "metadata": {},
   "source": [
    "Limitations of HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"HDFS can directly retrieve(low latency access) a portion of the data blocks from the DataNodes if you only need information from that part.\" \n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a97d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68215154",
   "metadata": {},
   "source": [
    "Limitations of HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"HDFS is not optimised to handle small files efficiently because the metadata created will be too much to handle for the NameNode.\"\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3fc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f39f18",
   "metadata": {},
   "source": [
    "# MapReduce Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f3f49",
   "metadata": {},
   "source": [
    "## Introduction to MapReduce Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e58cfb",
   "metadata": {},
   "source": [
    "MapReduce Framework\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The Mapper is responsible for reading the data and then forming key-value pairs according to the task that needs to be performed.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc86f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a40782c",
   "metadata": {},
   "source": [
    "MapReduce Framework\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The key-value pairs that have the same value in all the fields are treated as equal in the Map phase.\"\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff848f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46295c40",
   "metadata": {},
   "source": [
    "## Basic Implementation of MapReduce using Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e69e5",
   "metadata": {},
   "source": [
    "Basic Implementation of MapReduce using Python\n",
    "What is the purpose of using the ‘sys’ library of Python in MapReduce scripts?\n",
    "\n",
    "\n",
    "It is used for taking the inputs from the data set and the print statements used in the scripts.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5fb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96fd4849",
   "metadata": {},
   "source": [
    "Basic Implementation of MapReduce using Python\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The Mapper script outputs the data in the form of tuples.\"\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015a41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc6ea97",
   "metadata": {},
   "source": [
    "Basic Implementation of MapReduce using Python\n",
    "Why is a dictionary data structure used in the Reducer script?\n",
    "\n",
    "\n",
    "It is used for creating separate lists for different keys and their corresponding values.\n",
    "\n",
    "\n",
    "It is used for aggregating the data and performing the required functions and jobs efficiently.\n",
    "\n",
    "\n",
    "Both of the above options.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad572d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57cf6380",
   "metadata": {},
   "source": [
    "## Hadoop Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e39c38",
   "metadata": {},
   "source": [
    "Hadoop Streaming\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The input data set should be moved into the HDFS before using the Hadoop Streaming command.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799c05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "befa2d71",
   "metadata": {},
   "source": [
    "Hadoop Streaming\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"When viewing the result of a MapReduce job in HDFS, the ‘cat’ command is used to print out the content from all the files in the output directory whose names start with ‘part’, thereby printing out the entire result of the corresponding MapReduce program.\"\n",
    "\n",
    "\n",
    "True   ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61a5b63",
   "metadata": {},
   "source": [
    "Hadoop Streaming\n",
    "What will happen if the output directory already exists for a MapReduce job?\n",
    "\n",
    "\n",
    "The job will throw an error stating that the output directory already exists.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8962fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "327c645a",
   "metadata": {},
   "source": [
    "## The Combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0048d2",
   "metadata": {},
   "source": [
    "Combiner\n",
    "Recall the example from the previous segment, where you had to find the maximum age corresponding to each gender in the Customer Age data set and answer the following questions:\n",
    "\n",
    "Will a Combiner help in such a case?\n",
    " \n",
    "\n",
    "\n",
    "Yes  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc3d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef6904b",
   "metadata": {},
   "source": [
    "Combiner\n",
    "Recall the example from the previous segment, where you had to find the maximum age corresponding to each gender in the Customer Age data set and answer the following questions:\n",
    "\n",
    "How will a Combiner help in performing this job? (Note: Multiple options may be correct.)\n",
    "\n",
    "\n",
    "By finding the max for all the key-value pairs in each output of a Map task and, thus, improving the performance of the MapReduce job.    ✓ Correct\n",
    "\n",
    "By reducing memory usage of the Reduce task.    ✓ Correct\n",
    "\n",
    "By reducing the Network I/O latency.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9efdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "867f6bad",
   "metadata": {},
   "source": [
    "Combiner\n",
    "Recall the example from the previous segment, where you had to find the maximum age corresponding to each gender in the Customer Age data set and answer the following questions:\n",
    "\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"The Reducer, with a few small modifications, can be taken as reference for implementing the Combiner for this job.\"\n",
    "\n",
    "\n",
    "True     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b5180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeb56c6f",
   "metadata": {},
   "source": [
    "## The Partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c9263",
   "metadata": {},
   "source": [
    "Partitioner\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"By enabling similar data to be present and processed in the same job, the Partitioner helps in reducing the time taken for processing a job.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc807556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0224e4eb",
   "metadata": {},
   "source": [
    "## Job Scheduling and Fault Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583f253",
   "metadata": {},
   "source": [
    "HDFS\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "\"Each node in a cluster has multiple containers that are running a Map or a Reduce task.\"\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41abdbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6cf069",
   "metadata": {},
   "source": [
    "MapReduce Framework\n",
    "Is it possible to increase the number of Map tasks for a MapReduce job?\n",
    "\n",
    "\n",
    "Yes    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92972240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "406b64aa",
   "metadata": {},
   "source": [
    "MapReduce Framework\n",
    "The number of Map tasks equals the number of input file splits, and it may be increased by the user by reducing the input split size. This leads to ____.\n",
    "\n",
    "\n",
    "Improved resource utilisation    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991892a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be912eaa",
   "metadata": {},
   "source": [
    "YARN\n",
    "Which of the following has not improved due to the decoupling of resource management and scheduling components of Hadoop?\n",
    "\n",
    "\n",
    "Flexibility\n",
    "\n",
    "\n",
    "Fault Tolerance\n",
    "\n",
    "\n",
    "None of the above    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3cf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff3eaee",
   "metadata": {},
   "source": [
    "# MapReduce Programming Coding Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439663d",
   "metadata": {},
   "source": [
    "MapReduce Programming-01\n",
    "Download this data set containing the complete works of William Shakespeare and then write MapReduce programs to find the following:\n",
    "\n",
    "Which word has the highest frequency of occurrence in the document?\n",
    "\n",
    "\n",
    "‘the’ (30,086 times)    ✓ Correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5700a",
   "metadata": {},
   "source": [
    "MapReduce Programming-02\n",
    "Download this data set containing the complete works of William Shakespeare and then write MapReduce programs to find the following:\n",
    "\n",
    "What is the frequency of occurrence of the word ‘Romeo’? (Ignore cases and don't remove punctuation marks from any words.)\n",
    "\n",
    "\n",
    "48\n",
    "\n",
    "\n",
    "49      ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4d2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17cbed39",
   "metadata": {},
   "source": [
    "MapReduce Programming-03\n",
    "Download this data set containing the complete works of William Shakespeare and then write MapReduce programs to find the following:\n",
    "\n",
    "What is the frequency of the phrase \"circumference.\" in the data set? (You do not need to remove the punctuation marks from the words.)\n",
    "\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "2    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f46aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bad557d",
   "metadata": {},
   "source": [
    "MapReduce Programming-04\n",
    "Download this airline data set and then create MapReduce programs to find the following:\n",
    "\n",
    "Count the number of unique ordered pairs of origin and destination (Origin, Destination) present in the dataset, i.e., for two flights, either the origin or the destination differs.\n",
    "\n",
    "\n",
    "1024\n",
    "\n",
    "\n",
    "1023\n",
    "\n",
    "\n",
    "1403\n",
    "\n",
    "\n",
    "1404     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f7a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6faef632",
   "metadata": {},
   "source": [
    "MapReduce Programming-05\n",
    "Download this airline data set and then create MapReduce programs to find the following:\n",
    "\n",
    "What is the airport code and the number of flights corresponding to that airport, with the maximum number of outgoing flights in the year 2004?\n",
    "\n",
    "\n",
    "ORD, 5320    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd0d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6d8583f",
   "metadata": {},
   "source": [
    "MapReduce Programming-06\n",
    "Download this ODI batting data set and then create MapReduce programs to find the following:\n",
    "\n",
    "Which player scored the highest number of centuries?\n",
    "\n",
    "\n",
    "Sachin R Tendulkar (49 centuries)\n",
    "\n",
    "\n",
    "Sachin R Tendulkar (48 centuries)    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd67a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4f4250a",
   "metadata": {},
   "source": [
    "MapReduce Programming-07\n",
    "Download this ODI batting data set and then create MapReduce programs to find the following:\n",
    "\n",
    "In which year did Indian players score the maximum number of centuries?\n",
    "\n",
    "\n",
    "1992\n",
    "\n",
    "\n",
    "2011\n",
    "\n",
    "\n",
    "1999\n",
    "\n",
    "\n",
    "1998   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349deb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bdeb1c5",
   "metadata": {},
   "source": [
    "# Graded Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d5f943",
   "metadata": {},
   "source": [
    "Q1 Hadoop\n",
    "Suppose Ram has to implement an alternative server architecture in his organisation, as its single server system is no longer capable of satisfying the growing needs and is also limited in terms of scalability. Ram decides to use conventional desktop systems to implement a Distributed Hadoop Cluster System. What do you think would be the primary reason behind Ram deciding to use this kind of implementation?\n",
    "\n",
    "\n",
    "Commodity machines are cheap and available easily.    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4432707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbd893b0",
   "metadata": {},
   "source": [
    "Q2 Hadoop Ecosystem\n",
    "Consider the case of a children’s hospital. This hospital is completely digitised, in the sense that all reports, such as scan reports, blood test reports, etc, are stored as digital data, which is then stored in a Hadoop Cluster implementation. The hospital uses a lot of machines connected over a network, and, hence, a large volume of structured as well as unstructured real-time data is streamed to its Hadoop cluster. The hospital is not able to understand how to process this data. Which specific tool of the Hadoop ecosystem can help the hospital in this situation?\n",
    "\n",
    "\n",
    "Flume\n",
    "\n",
    "\n",
    "Sqoop\n",
    "\n",
    "\n",
    "Spark Streaming    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d4c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f309156",
   "metadata": {},
   "source": [
    "Q3 MapReduce Framework\n",
    "Which of the following takes (key-value) pairs as input and produces output that is also in the form of (key-value) pairs?\n",
    "\n",
    "\n",
    "Mapper\n",
    "\n",
    "\n",
    "Reducer\n",
    "\n",
    "\n",
    "Both Mapper and Reducer    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81ca7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a996b6f6",
   "metadata": {},
   "source": [
    "Q4 HDFS\n",
    "A client wants to read a block that is available in three DataNodes present across multiple geographical locations. The distances between the client and the DataNodes are as follows:\n",
    "\n",
    "DataNode 1: 5 units\n",
    "\n",
    "DataNode 2: 4 units\n",
    "\n",
    "DataNode 3: 7 units\n",
    "\n",
    "What would be the order of preference followed by the NameNode to access the block?\n",
    "\n",
    "\n",
    "1-2-3\n",
    "\n",
    "\n",
    "2-1-3   ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21752af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ed53267",
   "metadata": {},
   "source": [
    "Q5 MapReduce Job\n",
    "Consider a MapReduce problem where we need to find out the average of the marks scored by different students corresponding to each class. The data is in the form of a CSV file with each entry corresponding to a particular student and has two comma-separated columns, the first one containing the class of a student and the second one containing their marks.\n",
    "\n",
    "The code for the Mapper script is as follows:\n",
    "\n",
    "import sys\n",
    "#input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split(\";\")\n",
    "\n",
    "    if len(line) <=2:\n",
    "        key = line[0]\n",
    "        value = line[1]\n",
    "\n",
    "        print ('%s\\t%s' % (key, value))\n",
    "Is there any problem with the code above?\n",
    "\n",
    "If Yes, then what is the problem with this code with respect to the problem at hand?\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "The code needs no change.\n",
    "\n",
    "\n",
    "There is a problem in assigning the key and value variables.\n",
    "\n",
    "\n",
    "There is a problem in the split statement.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f88d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9ff4c2",
   "metadata": {},
   "source": [
    "Q6 Hadoop Architecture\n",
    "In a Hadoop cluster, if the NameNode (master node) fails, which of the following nodes takes up its responsibilities?\n",
    "\n",
    "\n",
    "Secondary NameNode\n",
    "\n",
    "\n",
    "DataNode\n",
    "\n",
    "\n",
    "Passive/Standby NameNode    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7de749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d5ed063",
   "metadata": {},
   "source": [
    "Q7 Rack Awareness in Hadoop\n",
    "Allen has access to three data centres, and he has stored his data in such a way that for each block of data, 2 out of the 3 replicas are stored in a single data centre.\n",
    "\n",
    "What would be the major benefit of this approach?\n",
    "\n",
    "\n",
    "It reduces the network I/O for read and write operations.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c773f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb8eb1e",
   "metadata": {},
   "source": [
    "Q8 HDFS\n",
    "Suppose there is a Hadoop Cluster that contains 1,000 files of size 2 MB each, with the chunk size equal to the file size, i.e., 2 MB. The size of the metadata for storing information for a single chunk in the system is 10 KB. Suppose there is another Hadoop cluster where the same data has been stored in larger chunks of size 200 MB each. What would be the respective storage space occupied for storing the metadata of these files in each of these clusters?\n",
    "\n",
    "Note: The size of the metadata for a single chunk is the same for both systems.\n",
    "\n",
    "\n",
    "9.76 MB, 500 KB\n",
    "\n",
    "\n",
    "9.76 MB, 1,000 KB\n",
    "\n",
    "\n",
    "9.76 MB, 100 KB    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971001a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78063d8e",
   "metadata": {},
   "source": [
    "Q9 MapReduce Framework\n",
    "Determine whether the following statement is True or False.\n",
    "\n",
    "“The final outputs of Reduce tasks are stored on the local file system of the data nodes running them”.\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988671b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa2dc78",
   "metadata": {},
   "source": [
    "Q10 MapReduce Framework\n",
    "Determine whether the following statement is True or False?\n",
    "\n",
    "“The final outputs of Map tasks are stored on the local file system of the data nodes running them”.\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1bae93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c30cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16a7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48e2d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"ashutoshgole18/data-engineering-module-5-introduction-to-hadoop-and-mapreduce-programming\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/ashutoshgole18/data-engineering-module-5-introduction-to-hadoop-and-mapreduce-programming\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ashutoshgole18/data-engineering-module-5-introduction-to-hadoop-and-mapreduce-programming'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e3b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
