{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86cb82a",
   "metadata": {},
   "source": [
    "# Traditional Warehouse Vs. Amazon Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6856b",
   "metadata": {},
   "source": [
    "## Recap: Data Warehousing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59446fb",
   "metadata": {},
   "source": [
    "OLTP\n",
    "Which of the following strategies are followed by OLTP (relational databases) systems for a large database?\n",
    "\n",
    "\n",
    "Normalisation   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54cd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c71053d5",
   "metadata": {},
   "source": [
    "Big Data\n",
    "Why do we need specialised Big Data storing/processing tools in the first place?\n",
    "\n",
    "\n",
    "To provide more security to data\n",
    "\n",
    "\n",
    "To store a bulk amount of data    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2974e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0bed3ab",
   "metadata": {},
   "source": [
    "## On-Premise vs Cloud Data Warehouses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9377f9f",
   "metadata": {},
   "source": [
    "Fill in the blank\n",
    "Data warehouse databases use a different type of architecture both from a ________ perspective and ________layer.\n",
    "\n",
    "\n",
    "End-user, administrativ   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908b24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0405f62f",
   "metadata": {},
   "source": [
    "Not a kind\n",
    "A data warehouse is NOT used for?\n",
    "\n",
    "\n",
    "Transaction Processing   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c795cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c8abc2",
   "metadata": {},
   "source": [
    "## Why Amazon Redshift?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b20b5",
   "metadata": {},
   "source": [
    "Redshift Origin\n",
    "Redshift is a customised form of which database?\n",
    "\n",
    "\n",
    "PostgreSQL  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178fc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43064208",
   "metadata": {},
   "source": [
    "Redshift Database Service\n",
    "Amazon Redshift is a _________database service.\n",
    "\n",
    "\n",
    "Relational   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f638cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfd2964",
   "metadata": {},
   "source": [
    "Scaling Redshift\n",
    "Why Amazon Redshift using concurrency scaling?\n",
    "\n",
    "\n",
    "To increase workload\n",
    "\n",
    "\n",
    "To increase performance   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cba09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15809ae2",
   "metadata": {},
   "source": [
    "Integration with Redshift\n",
    "Which type of tool can be integrated with Redshift?\n",
    "\n",
    "\n",
    "Artificial Intelligence\n",
    "\n",
    "\n",
    "Business Intelligence   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e571f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83e139fa",
   "metadata": {},
   "source": [
    "# Redshift: Introduction and Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca182323",
   "metadata": {},
   "source": [
    "## Introduction to Amazon Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a37fc",
   "metadata": {},
   "source": [
    "Redshift Architecture\n",
    "What are the key reasons for choosing a data warehouse solution such as Redshift?\n",
    "(Note: More than one option may be correct.)\n",
    "\n",
    "\n",
    "High volume   ✓ Correct\n",
    "\n",
    "Supports stored procedures   ✓ Correct\n",
    "\n",
    "Massive parallel processing   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9df1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a76839d3",
   "metadata": {},
   "source": [
    "Redshift Performance\n",
    "Which of the following features is/are the reason(s) for Redshift’s fast performance?\n",
    "\n",
    "\n",
    "Columnar data stores\n",
    "\n",
    "\n",
    "Massively Parallel processing (MPP)\n",
    "\n",
    "\n",
    "Data compression\n",
    "\n",
    "\n",
    "All of the above  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d56abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c934413d",
   "metadata": {},
   "source": [
    "Redshift Feature\n",
    "Which feature was added by Amazon to Postgres, so that it will become a cloud-based data warehouse?\n",
    "\n",
    "\n",
    "Infrastructure as a Service (IaaS)\n",
    "\n",
    "\n",
    "Software as a Service (SaaS)\n",
    "\n",
    "\n",
    "Platform as a Service (PaaS)   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44e622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214926df",
   "metadata": {},
   "source": [
    "## Redshift Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e109c",
   "metadata": {},
   "source": [
    "Redshift Architecture\n",
    "Which of the following nodes is responsible for managing client connections?\n",
    "\n",
    "\n",
    "Client node\n",
    "\n",
    "\n",
    "Leader node  ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90236cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fdda57a",
   "metadata": {},
   "source": [
    "Redshift Architecture\n",
    "Select the correct statement from below.\n",
    "\n",
    "\n",
    "The leader node computes and executes queries and compute nodes calculate these queries.\n",
    "\n",
    "\n",
    "The leader node handles connections, &  compute nodes stores data and performs calculations.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb7c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e75f0a77",
   "metadata": {},
   "source": [
    "Connection\n",
    "How do the end-users applications connect with Redshift? \n",
    " \n",
    "\n",
    "\n",
    "Spring DAO\n",
    "\n",
    "\n",
    "ODBC  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc7320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144dc2eb",
   "metadata": {},
   "source": [
    "## Key Performance Features of Redshift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd879fb6",
   "metadata": {},
   "source": [
    "Key Performance Features of Redshift\n",
    "Why is Redshift built with columnar based storage and not row-based storage?\n",
    "\n",
    "\n",
    "It optimises hardware.\n",
    "\n",
    "\n",
    "It gives proficiency in queries.\n",
    "\n",
    "\n",
    "It is the only way for Redshift.\n",
    "\n",
    "\n",
    "It reduces disk I/O for analytical queries.  ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366e638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "734255c2",
   "metadata": {},
   "source": [
    "Columnar Database\n",
    "A columnar database is best suited for _________ \n",
    "\n",
    "\n",
    "Transactional processing\n",
    "\n",
    "\n",
    "Analytical query processing   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812f37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c1b67cc",
   "metadata": {},
   "source": [
    "## SORT Key II & ZONE Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9aa21",
   "metadata": {},
   "source": [
    "Sort Keys\n",
    "Which of the following is the default Sort key type in Amazon Redshift?\n",
    "\n",
    "\n",
    "Compound sort key  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18b2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d607ee81",
   "metadata": {},
   "source": [
    "Fill in the blank.\n",
    "______________sort key gives equal weightage to each column.\n",
    "\n",
    "\n",
    "Compound\n",
    "\n",
    "\n",
    "Interleaved  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b1f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3113e09a",
   "metadata": {},
   "source": [
    "System Table\n",
    "Which of the following system tables contains information about the Sort key?\n",
    "Note: Please go through this Additional Reading link to answer this question. \n",
    "\n",
    "\n",
    "ALL_OBJ_TABLE\n",
    "\n",
    "\n",
    "CONST_KEY_TABLE\n",
    "\n",
    "\n",
    "SVV_TABLE_INFO  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc57f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67f0ec4c",
   "metadata": {},
   "source": [
    "Sort key\n",
    "Which of the following types of the column will be the most efficient when it comes to choosing one as the SORT key? \n",
    "\n",
    "\n",
    "TIMESTAMP ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524e24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25aa318",
   "metadata": {},
   "source": [
    "## Data Distribution: DIST Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80568e",
   "metadata": {},
   "source": [
    "Distribution Key\n",
    "Select the condition for using the EVEN distribution key from below.\n",
    "\n",
    "\n",
    "Tables should not participate in Joins. ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dd6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df935f8b",
   "metadata": {},
   "source": [
    "Distribution Key\n",
    "What are the conditions for choosing DISTKEY/SORTKEY?\n",
    "\n",
    "\n",
    "The nature of data\n",
    "\n",
    "\n",
    "The nature of the query\n",
    "\n",
    "\n",
    "Both (A) and (B)  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc1160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b044c60",
   "metadata": {},
   "source": [
    "# Redshift Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309561bd",
   "metadata": {},
   "source": [
    "## Redshift Cluster: Node Types & Maintenance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe842b3",
   "metadata": {},
   "source": [
    "Redshift Cluster\n",
    "What is the default port of Redshift Database cluster?\n",
    "\n",
    "\n",
    "5431\n",
    "\n",
    "\n",
    "5432\n",
    "\n",
    "\n",
    "5439  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f5dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "857c666a",
   "metadata": {},
   "source": [
    "Redshift Cluster\n",
    "If you have selected the dc2.large(Take the hourly rates for this instance type as 0.25$ per hour) as a node type and created a two-node Redshift cluster, then what will be the cost for a year. \n",
    "Consider the region as US East (N. Virginia)\n",
    "\n",
    "$4380 ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1d569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "469b98ce",
   "metadata": {},
   "source": [
    "## Workload Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacce643",
   "metadata": {},
   "source": [
    "Redshift WLM\n",
    "Workload of the compute nodes can be configured using __________________\n",
    "\n",
    "\n",
    "Workload Management  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5bbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5357c59d",
   "metadata": {},
   "source": [
    "Redshift WLM\n",
    "What does Automatic Workload Management do? Select from the following options:\n",
    "\n",
    "\n",
    "Automatically pauses cluster after a certain period of time\n",
    "\n",
    "\n",
    "Automatically cleans all caches from Redshift memory\n",
    "\n",
    "\n",
    "Automatically calculates the percentage of memory assign to the default queue  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59509b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b05efdb",
   "metadata": {},
   "source": [
    "Redshift WLM\n",
    "Which node is best for performance-intensive workloads?\n",
    "\n",
    "\n",
    "Dense Leader Node\n",
    "\n",
    "\n",
    "Dense Cluster Node\n",
    "\n",
    "\n",
    "Dense Storage Node\n",
    "\n",
    "\n",
    "Dense Compute Node  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90c2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b1feac",
   "metadata": {},
   "source": [
    "## Fault Tolerance and Security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557561e6",
   "metadata": {},
   "source": [
    "Feature\n",
    "Which of the following node types would you suggest to store the data for the use case having data growing rapidly day by day?\n",
    "\n",
    "\n",
    "RA3   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915ae8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "709ffd1e",
   "metadata": {},
   "source": [
    "Feature\n",
    "Your company is currently using Oracle database and you will get a bulk amount of blob type data daily. This is very tough to handle in a traditional data warehouse. So, as a solution architect, you proposed Amazon Redshift to your organisation which is fully scalable, offers robust query performance and is cost-effective. Why did you choose Redshift?\n",
    "\n",
    "\n",
    "Redshift built on top of Postgres which is helpful to Oracle developers for easy adaption.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e9afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fbeb3d5",
   "metadata": {},
   "source": [
    "Workload Management\n",
    "One of the queries you are running on a Redshift cluster needs more resources to run. What is the correct way to allocate more resources to a query?\n",
    "Note: Please go through this additional reading link before answering this question. \n",
    "\n",
    "\n",
    "Set wlm_query_slot_count to 3;  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22944351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7bc0762",
   "metadata": {},
   "source": [
    "# Redshift Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f460592",
   "metadata": {},
   "source": [
    "## Getting Started With Redshift Queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc4621",
   "metadata": {},
   "source": [
    "Primary key\n",
    "Does Redshift have a primary key and foreign key constraints?\n",
    " \n",
    "\n",
    "\n",
    "Yes\n",
    "\n",
    "\n",
    "No ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137359c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9b57753",
   "metadata": {},
   "source": [
    "Schema\n",
    "Suppose you are a database developer and have good experience in Redshift. Now a task is assigned to you to find a schema of a table from Redshift Database cluster. That table name is finscheduledetail under schema finance. Select possible option(s) for correct query.\n",
    "\n",
    "\n",
    "select * from information_schema.columns where table_name='finscheduledetail'\n",
    "And table_schema='finance'   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0621b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ee0254",
   "metadata": {},
   "source": [
    "## Loading Data Into Redshift Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08d238",
   "metadata": {},
   "source": [
    "Copy Command\n",
    "Suppose a table named student is present in a Redshift cluster inside a schema named school. You have been asked to load data into this student table from a 10GB text file present in a S3 bucket. Which of the following formats of the Copy command will you be making use of? Note: We don't know the format of the text file.\n",
    "\n",
    "\n",
    "copy student from “s3://<path>” iam_role ‘<roleid> ‘ delimiter ‘|’ region ‘us-east-1’;\n",
    "\n",
    "\n",
    "copy student from “s3://<path>” iam_role ‘<roleid> ‘ delimiter ‘,’ region ‘us-east-1’;\n",
    "\n",
    "\n",
    "copy school.student from “s3://<path>” iam_role ‘<roleid> ‘ delimiter ‘|’ region ‘us-east-1’;\n",
    "\n",
    "\n",
    "Inadequate information   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6dd305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef0a16a5",
   "metadata": {},
   "source": [
    "Copy Command\n",
    "Does CSV or TSV file formats provide significant performance improvements while using the Copy command?\n",
    "\n",
    "\n",
    "Yes\n",
    "\n",
    "\n",
    "No   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d998f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0472ea09",
   "metadata": {},
   "source": [
    "COPY Command\n",
    "Should the number of input files to COPY command always be in the multiple of the number of node slices?\n",
    "\n",
    "Consider an optimal case. \n",
    "\n",
    "\n",
    "Yes   ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1f4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1035071f",
   "metadata": {},
   "source": [
    "Data Loading\n",
    "Your daily activity is to monitor the COPY command which is loading data into your Redshift database cluster and to provide analytic insights.  During a routine monitoring exercise suppose you have found one of the COPY commands showing success but data is not present in the respective table. What is/are the possible reason/s?\n",
    " \n",
    "\n",
    "\n",
    "IAM role associated with your Redshift cluster has invalid policy attached    ✓ Correct\n",
    "\n",
    "S3 bucket name may be incorrect at policy    ✓ Correct\n",
    "\n",
    "File which you have mentioning in COPY command is not available at S3 bucket    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c048cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fffd316f",
   "metadata": {},
   "source": [
    "# Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d9d53",
   "metadata": {},
   "source": [
    "Q1 Age-group\n",
    "How many people are suspected of COVID-19 and belong to the 20-49 year age group?\n",
    "\n",
    "\n",
    "62.14 %  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb429dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01cdaea",
   "metadata": {},
   "source": [
    "Q2 Maximum Case\n",
    "Which state has the maximum confirmed case of COVID-19?\n",
    "\n",
    "\n",
    "Delhi\n",
    "\n",
    "\n",
    "Maharashtra  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa6e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73844a14",
   "metadata": {},
   "source": [
    "Q3 Individual Details\n",
    "According to our data set, how many people have travelled from Wuhan to India and got recovered from COVID-19?\n",
    "\n",
    "\n",
    "3  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396742fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7218311c",
   "metadata": {},
   "source": [
    "Q4 Death Count\n",
    "Which state has the second maximum death count?\n",
    "\n",
    "\n",
    "Telangana\n",
    "\n",
    "\n",
    "Uttar Pradesh\n",
    "\n",
    "\n",
    "Delhi  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9662fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6411c9b0",
   "metadata": {},
   "source": [
    "Q5 Recovered\n",
    "How many people recovered whose age is equal to 55?\n",
    "\n",
    "\n",
    "21\n",
    "\n",
    "\n",
    "15\n",
    "\n",
    "\n",
    "10\n",
    "\n",
    "\n",
    "12  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127c7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657365cf",
   "metadata": {},
   "source": [
    "Q6 State Cure Count\n",
    "Which State has cured the minimum number of people?\n",
    "\n",
    "\n",
    "Dadra & Nagar Haveli \n",
    "\n",
    "\n",
    "Sikkim   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da18cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd535ea",
   "metadata": {},
   "source": [
    "# Advanced Features and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a72d3a",
   "metadata": {},
   "source": [
    "## Redshift Spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f372ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d7be7f",
   "metadata": {},
   "source": [
    "Redshift Spectrum\n",
    "Redshift spectrum layer query the data which is present on ____________________\n",
    "\n",
    "\n",
    "Simple Storage Service(Amazon S3)  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c35c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486123d0",
   "metadata": {},
   "source": [
    "File Format\n",
    "Which one of the following file formats is NOT supported by Redshift Spectrum?\n",
    "\n",
    "\n",
    "CSV\n",
    "\n",
    "\n",
    "TIFF ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f535345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f5f9232",
   "metadata": {},
   "source": [
    "Redshift Query\n",
    "What is the maximum size of a file for processing by Redshift Spectrum in Amazon S3?\n",
    "\n",
    "\n",
    "Exabyte ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d0a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d723c1e",
   "metadata": {},
   "source": [
    "Redshift Spectrum\n",
    "Your company is using an on-premise data warehouse and wants to move to AWS. They have 10 years of data (about 2 Petabytes) currently. They have complex analytical dashboards which queries most recent 1-year data. Though they should be able query data older than 1-year as well (very rarely). What solution would you recommend?\n",
    "\n",
    "\n",
    "Migrate all data to Amazon S3. Query all data using Athena \n",
    "\n",
    "\n",
    "Migrate the data to Amazon S3. Move the latest 1-year data into the Amazon Redshift cluster and use it for analysis. For older data, use Amazon Redshift Spectrum to query data which is present on S3. ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f8912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f21f8c1",
   "metadata": {},
   "source": [
    "## RA3 and AQUA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba63c44",
   "metadata": {},
   "source": [
    "AQUA\n",
    "In AQUA, cache is_________________.\n",
    " \n",
    "\n",
    "\n",
    "Distributed   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb677cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd040bf",
   "metadata": {},
   "source": [
    "AQUA\n",
    "How does AQUA boost the performance of queries while executing in the Redshift cluster?\n",
    "\n",
    "\n",
    "Using hardware-accelerated cache\n",
    "\n",
    "\n",
    "Using distributed cache\n",
    "\n",
    "\n",
    "Using distributed and hardware-accelerated cache  ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977a038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2828a182",
   "metadata": {},
   "source": [
    "AQUA\n",
    "AQUA provides additional boosting performance to query execution. Which of the following types of disk drive is recommended for storing & querying bulk amounts of data in Redshift cluster?\n",
    "\n",
    "\n",
    "HDD\n",
    "\n",
    "\n",
    "SSD   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cab56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd0afcc",
   "metadata": {},
   "source": [
    "AQUA\n",
    "You are managing a Redshift cluster with one database which is being used by multiple teams within an organization. Currently, all tables/views reside in the public schema. Over time, the data has become cluttered and you have received a lot of complaints from some teams that some other team deleted their data. \n",
    "\n",
    "What is the quickest and cost-efficient to implement governance around the data, such that one team cannot access data that belongs to another team?\n",
    " \n",
    "\n",
    "\n",
    "Create separate WLM queues for different teams\n",
    "\n",
    "\n",
    "Create separate schemas for each team   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f59a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b5fc985",
   "metadata": {},
   "source": [
    "# Graded Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54d362",
   "metadata": {},
   "source": [
    "Q1 COPY command\n",
    "You are working for a Pharma company which receives data from multiple external vendors for processing. You are receiving the data in large files (several GBs) stored on S3. Each night, COPY command runs to load data into a 5-node DC2.large Redshift cluster. You found that the COPY command is taking a long time to complete.\n",
    "\n",
    "How would you optimise the data load process?\n",
    "\n",
    "\n",
    "Split the data files into 1 GB each, compress them and load them using a COPY command with GZIP   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f97b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f77a6f7",
   "metadata": {},
   "source": [
    "Q2 Redshift Query Optimisation\n",
    "The Redshift cluster you are managing for your company has a variety of workloads. There are users who run long, complex analytical queries as well as some automated jobs which run short, read-only queries. You are observing that during high workload, these short-running queries are timing out and the related jobs are failing.\n",
    "Which solution is the simplest one to remediate this problem?\n",
    " \n",
    "\n",
    "\n",
    "Use automated Workload Management with concurrency scaling\n",
    "\n",
    "\n",
    "Add more nodes to your Redshift cluster using Elastic Resize\n",
    "\n",
    "\n",
    "Use manual Workload Management with concurrency scaling\n",
    "\n",
    "\n",
    "Use Short Query Acceleration   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2268e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a31ce17",
   "metadata": {},
   "source": [
    "Q3 DW Use Case\n",
    "You have large volumes of structured data in the form of comma-delimited (CSV) files on your on-premise shared storage drive. You have been asked to migrate the data to AWS. The data must be queried using standard SQL. \n",
    "\n",
    "Which solution will meet the requirements?\n",
    " \n",
    "\n",
    "\n",
    "Use COPY command to load data in parallel into Hive on an EMR cluster from S3\n",
    "\n",
    "\n",
    "Use INSERT command to load data in parallel into Redshift cluster from S3\n",
    "\n",
    "\n",
    "Use COPY command to load in parallel into Redshift cluster from S3   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2dbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31282403",
   "metadata": {},
   "source": [
    "Q4 Monitoring Redshift\n",
    "You are managing the Redshift database for a company and you have a requirement to set up monitoring on the Redshift cluster and send email notifications whenever the CPU Utilization for your cluster crosses 90%. \n",
    "\n",
    "Which of the following services would you use to meet this requirement? \n",
    "\n",
    "\n",
    "Amazon CloudWatch   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c78ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "324718b1",
   "metadata": {},
   "source": [
    "Q5 Redshift Performance\n",
    "You are working for an insurance company which uses 3-node DS2.XLARGE Redshift cluster to store claims data. There are some BI dashboards which query this data and show some key metrics such as total claim value and the number of claims. These dashboards are updated every hour through SQL queries. There is also a group of data scientists who query the database intermittently to analyse risks of some claims. Recently, the data scientists have complained of slow queries.\n",
    "\n",
    "What will be the most cost-effective solution to increase the performance of your Redshift cluster?\n",
    "\n",
    "\n",
    "Change the Redshift storage to provisioned IOPS(Input/Output Operations per Second) to increase the I/O \n",
    "\n",
    "\n",
    "Create separate Redshift cluster for data scientists and ask them to use that for their queries\n",
    "\n",
    "\n",
    "Change node type of Redshift cluster to DC2.XLARGE using Elastic Resize.\n",
    "\n",
    "\n",
    "Create separate WLM queue for data scientists and the BI dashboards and configure Automatic WLM   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d8cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49cf07c2",
   "metadata": {},
   "source": [
    "Q6 EXPLAIN Command\n",
    "You are working on tuning the performance on an SQL query on Redshift cluster. You generated an EXPLAIN plan for this query and see the following output:\n",
    "\n",
    "Query:\n",
    "\n",
    "explain select eventid, eventname, event.venueid, venuename from event, venue where event.venueid = venue.venueid\n",
    "Output:\n",
    "\n",
    "XN Hash Join DS_DIST_OUTER  (cost=2.52..58653620.93 rows=8712 width=43)\n",
    "Hash Cond: (\"outer\".venueid = \"inner\".venueid)\n",
    "->  XN Seq Scan on event  (cost=0.00..87.98 rows=8798 width=23)\n",
    "->  XN Hash  (cost=2.02..2.02 rows=202 width=22)\n",
    "->  XN Seq Scan on venue  (cost=0.00..2.02 rows=202 width=22)\n",
    "(519 rows)\n",
    "How would you tune this query?\n",
    "\n",
    "\n",
    "Change the distribution style of event table to ALL\n",
    "\n",
    "\n",
    "Change the distribution style of venue table to ALL   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a128908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c909869e",
   "metadata": {},
   "source": [
    "Q7 Redshift Fault Tolerance\n",
    "You are running a single-node Redshift cluster. What happens to the durability when one of the drives on this node fails due to a hardware issue?\n",
    "\n",
    "\n",
    "The node with failed drive will get replaced automatically and the cluster will be available for read and write immediately. The data on the failed drive will be recovered using backups.\n",
    "\n",
    "\n",
    "The failed drive will get replaced automatically and the cluster will be available for read and write immediately. The data on failed drive will be lost and cannot be recovered\n",
    "\n",
    "\n",
    "The failed drive will get replaced automatically and the cluster will be available for read and write immediately. The data on failed drive will recovered using backups on S3\n",
    "\n",
    "\n",
    "You will have to manually create a new cluster from one of the prior snapshots. The data lost will not be recovered.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a2119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1880368e",
   "metadata": {},
   "source": [
    "Q8 Node Type\n",
    "You are working for an organization who were using a Redshift cluster with 3 ds2.xlarge nodes to store about 5 TB of raw data. They expect the data to grow further at a rate of 2 TB every month. They have a BI dashboard which makes use of all data present on the Redshift cluster. \n",
    "\n",
    "\n",
    "They are looking for an optimal solution to meet their increased data growth and storage requirement. Which solution will be the most efficient and cost-optimised? \n",
    "\n",
    "\n",
    "Add a new node to the Redshift cluster every month to store additional data.\n",
    "\n",
    "\n",
    "Offload some data to S3 and query using Redshift Spectrum\n",
    "\n",
    "\n",
    "Upgrade to RA3 node types using Elastic Resize   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16ca9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7af1f9cf",
   "metadata": {},
   "source": [
    "Q9 DIST Key\n",
    "You are using a Redshift cluster of 2-node ds2.xlarge type nodes (with 2 slices each). When you use DISTSTYLE ALL for a Redshift table, how is the data stored on compute nodes?\n",
    "\n",
    "\n",
    "Data is stored on the Master Node\n",
    "\n",
    "\n",
    "Data is stored on Compute Node-1 only\n",
    "\n",
    "\n",
    "Data is stored on all slices of all compute nodes\n",
    "\n",
    "\n",
    "Data is stored on first slice of all compute node   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb6c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e79620df",
   "metadata": {},
   "source": [
    "Q10 Redshift Cluster\n",
    "Your organization has just started using AWS Redshift for their data warehousing needs. They have determined that they will be using it for the next 3 years as per the current business plan. As a solutions architect, which of the following solutions would you suggest for saving on Redshift costs?\n",
    "\n",
    "\n",
    "Use on-demand Redshift instances for the cluster\n",
    "\n",
    "\n",
    "Use RA3 node types for the Redshift cluster\n",
    "\n",
    "\n",
    "Reserve Redshift nodes for 3 years using Redshift instance reservation  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549efe31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728c037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ee5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369404de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"ashutoshgole18/data-engineering-module-12-amazon-redshift\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/ashutoshgole18/data-engineering-module-12-amazon-redshift\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ashutoshgole18/data-engineering-module-12-amazon-redshift'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e530be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
