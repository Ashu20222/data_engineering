{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05b1c9d",
   "metadata": {},
   "source": [
    "# Getting Started with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efbc04",
   "metadata": {},
   "source": [
    "## Spark Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94f28a",
   "metadata": {},
   "source": [
    "Apache Spark\n",
    "Which of the following is not a key functionality of Spark?\n",
    "\n",
    "\n",
    "High Speed\n",
    "\n",
    "\n",
    "Ease of use\n",
    "\n",
    "\n",
    "Ability to build databases    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e1389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17076253",
   "metadata": {},
   "source": [
    "Apache Spark\n",
    "Which of the following statements is not correct?\n",
    "\n",
    "\n",
    "MapReduce is a data-processing framework.\n",
    "\n",
    "\n",
    "Apache Spark is a data-processing framework.\n",
    "\n",
    "\n",
    "HDFS is a data-processing framework.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0d4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a1afaa",
   "metadata": {},
   "source": [
    "Apache Spark\n",
    "Which of the following cluster managers is/are supported by Spark?\n",
    "\n",
    "\n",
    "Apache Mesos    ✓ Correct\n",
    "\n",
    "YARN    ✓ Correct\n",
    "\n",
    "Standalone Cluster Manager    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122e235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbc2c39",
   "metadata": {},
   "source": [
    "## Spark vs. MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb306fce",
   "metadata": {},
   "source": [
    "Spark vs MapReduce\n",
    "Choose whether the following statement is True or False:\n",
    "\n",
    "“Disk-based processing systems are cheaper and, hence, they can be scaled to a greater extent at a lower price when compared with in-memory processing”.\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66390f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949f13e9",
   "metadata": {},
   "source": [
    "Spark vs MapReduce\n",
    "Which of the following tools will be more suitable to host a ticket booking portal for a travel company website?\n",
    "\n",
    "\n",
    "Hadoop MapReduce\n",
    "\n",
    "\n",
    "Apache Spark    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32a198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8a33975",
   "metadata": {},
   "source": [
    "Spark vs MapReduce\n",
    "Choose whether the following statement is True or False.\n",
    "\n",
    "\n",
    "“To run Spark on an HDFS system, you need to perform some modification to the HDFS architecture”.\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597ebef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be5d5542",
   "metadata": {},
   "source": [
    "When to use Spark\n",
    "Which of the following sizes of a data set is Spark most ideally suited for? (Choose the most appropriate answer.)\n",
    "\n",
    "\n",
    "200–300 MB\n",
    "\n",
    "\n",
    "1–10 GB\n",
    "\n",
    "\n",
    "600–700 GB   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c101d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01c199b7",
   "metadata": {},
   "source": [
    "Spark vs MapReduce\n",
    "Which of the following has higher latency?\n",
    "\n",
    "\n",
    "Spark\n",
    "\n",
    "\n",
    "MapReduce   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6552d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94a91fb1",
   "metadata": {},
   "source": [
    "## Spark Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c267c3f",
   "metadata": {},
   "source": [
    "Apache Spark Ecosystem\n",
    "Which of the following provides an execution platform for Spark applications?\n",
    "\n",
    "\n",
    "Spark MLlib\n",
    "\n",
    "\n",
    "Spark Streaming\n",
    "\n",
    "\n",
    "Spark Core  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "392bde3a",
   "metadata": {},
   "source": [
    "Apache Spark Ecosystem\n",
    "In which form does the Spark Core engine process data?\n",
    "\n",
    "\n",
    "DataFrames\n",
    "\n",
    "\n",
    "Datasets\n",
    "\n",
    "\n",
    "RDDs  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef740a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44e41cfa",
   "metadata": {},
   "source": [
    "Apache Spark Ecosystem\n",
    "Is the following statement True or False?\n",
    "\n",
    "“The Dataframe API in Spark stores data in the form of rows and columns”.\n",
    "\n",
    "\n",
    "True  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff242dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c1c99dd",
   "metadata": {},
   "source": [
    "Apache Spark Ecosystem\n",
    "Which of the following is a library that comes bundled with Spark?\n",
    "\n",
    "\n",
    "Spark MLlib   ✓ Correct\n",
    "\n",
    "Spark JavaScript\n",
    "\n",
    "\n",
    "Spark SQL   ✓ Correct\n",
    "\n",
    "Spark R  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf757d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ecb0834",
   "metadata": {},
   "source": [
    "## Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ab52a",
   "metadata": {},
   "source": [
    "Master–Slave Architecture\n",
    "Which of the following processes runs over the slave node?\n",
    "\n",
    "\n",
    "Driver program\n",
    "\n",
    "\n",
    "Executor  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2ff45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fff82fd8",
   "metadata": {},
   "source": [
    "Master–Slave Architecture\n",
    "Which of the following statements about the Driver node is/are true?\n",
    "\n",
    "\n",
    "Driver node is the entry point of the Spark Shell environment.   ✓ Correct\n",
    "\n",
    "Driver node performs all the data processing involved in each task.\n",
    "\n",
    "\n",
    "Driver node is responsible for allocating the resources that are required to execute a task.\n",
    "\n",
    "\n",
    "Driver node stores the metadata of all the RDDs and their partitions.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10213fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faa8f2db",
   "metadata": {},
   "source": [
    "Spark Architecture\n",
    "Which of the following components of Spark runtime architecture provides resources to execute a task?\n",
    "\n",
    "\n",
    "Spark Context\n",
    "\n",
    "\n",
    "Driver program\n",
    "\n",
    "\n",
    "Cluster manager   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa3ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c9bebdd",
   "metadata": {},
   "source": [
    "Spark Architecture\n",
    "Choose whether the following statement is True or False:\n",
    "\n",
    "“A cluster manager is necessary to launch Spark.”\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648bff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59dc1c66",
   "metadata": {},
   "source": [
    "Spark Architecture\n",
    "“The final output of a Spark job is sent from the executors to the ____.”\n",
    "\n",
    "\n",
    "Cluster manager\n",
    "\n",
    "\n",
    "Driver node   ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f535844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0874a9f6",
   "metadata": {},
   "source": [
    "Spark Architecture\n",
    "Can we deploy Spark over Hadoop architecture?\n",
    "\n",
    "\n",
    "Yes   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c067f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c37e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dfa676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9915c07e",
   "metadata": {},
   "source": [
    "Spark APIs\n",
    "Which of the following APIs is useful for dealing with image or text files?\n",
    "\n",
    "\n",
    "Unstructured or low-level APIs   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc9ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176b4790",
   "metadata": {},
   "source": [
    "# Programming with Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406482d",
   "metadata": {},
   "source": [
    "## Introduction to Spark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7212b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark RDDs\n",
    "Can you change the data in an RDD once the RDD is created?\n",
    "\n",
    "\n",
    "Yes\n",
    "\n",
    "\n",
    "No   ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d2e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37880b47",
   "metadata": {},
   "source": [
    "Spark RDDs\n",
    "Can Spark RDDs be used to store and analyse structured data?\n",
    "\n",
    "\n",
    "Yes   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c76d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3016dc0",
   "metadata": {},
   "source": [
    "## Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c134d5",
   "metadata": {},
   "source": [
    "Creating RDDs\n",
    "Suppose you have a data set loaded in the Spark environment in the form of an array. Which of the following methods would be the most suitable to convert the data set into an RDD?\n",
    "\n",
    "\n",
    "textFile() method\n",
    "\n",
    "\n",
    "parallelize() method  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33bfaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8188f083",
   "metadata": {},
   "source": [
    "SparkSession\n",
    "What are the two necessary parameters to be specified at the time of initialising a SparkContext?\n",
    "\n",
    "\n",
    "Application name    ✓ Correct\n",
    "\n",
    "Number of executors provided for a Spark application\n",
    "\n",
    "\n",
    "The memory allocated to each executor\n",
    "\n",
    "\n",
    "Cluster mode    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1bd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b2c22e",
   "metadata": {},
   "source": [
    "Spark Programming\n",
    "Identify the correct syntax for saving an RDD externally on the disk.\n",
    "\n",
    "\n",
    "saveastextfile()\n",
    "\n",
    "\n",
    "SaveAsTextFile()\n",
    "\n",
    "\n",
    "saveAsTextFile()   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b5c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "436be462",
   "metadata": {},
   "source": [
    "## Transformation Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6e164",
   "metadata": {},
   "source": [
    "RDD Operations\n",
    "The input and output of a program are given below. Select the most appropriate function.\n",
    "\n",
    "input1=[1,2,3,4,5]\n",
    "rdd=spark.sparkContext.parallelize(input1)\n",
    "op1= (Select the correct option)\n",
    "op1.collect()\n",
    "OUTPUT: [(1,1),(2,1),(3,1),(4,1),(5,1)]\n",
    "\n",
    "\n",
    "op1=rdd.map(lambda x: (x,1))    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b8717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "600b2f4d",
   "metadata": {},
   "source": [
    "Apache Spark\n",
    "Which of the following statements is correct?\n",
    "\n",
    "\n",
    "The glom() function stores data inside RDDs as an external file.\n",
    "\n",
    "\n",
    "The glom() function creates a nested structure that stores data in each partition separately.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b43851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053a271b",
   "metadata": {},
   "source": [
    "RDD Operations\n",
    "Suppose there is an RDD ‘rdd1’, which represents a list of places where company A has its branches. There is another RDD ‘rdd2’, which represents a list of places where company B has its branches. Which of the following statements is correct?\n",
    "\n",
    "\n",
    "rdd1.subtract(rdd2) results in an RDD that contains a list of places where company A has its branches but company B does not.   ✓ Correct\n",
    "\n",
    "rdd1.subtract(rdd2) results in an RDD that contains a list of places where company B has its branches but company A does not.\n",
    "\n",
    "\n",
    "rdd1.intersection(rdd2) results in an RDD that contains a list of places where both company A and company B have their branches.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f873e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2949a1d3",
   "metadata": {},
   "source": [
    "RDD Operations\n",
    "Suppose there is an RDD ‘rdd1’, which represents a list of places where company A has its branches. There is another RDD ‘rdd2’, which represents a list of places where company B has its branches. Which of the following codes results in an RDD that contains all those places where company A and company B may have their branches?\n",
    "\n",
    "\n",
    "rdd1.union(rdd2)\n",
    "\n",
    "\n",
    "(rdd1.union(rdd2)).subtract(rdd1.intersection(rdd2))\n",
    "\n",
    "\n",
    "(rdd2.union(rdd1)).subtract(rdd2.intersection(rdd1))\n",
    "\n",
    "\n",
    "rdd1.union(rdd2).distinct()   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f1365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19aeeb01",
   "metadata": {},
   "source": [
    "## Action Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa28ee6",
   "metadata": {},
   "source": [
    "Transformations and Actions\n",
    "Suppose you have the following RDD.\n",
    "rdd1 = sc.parallelize(['upGrad', 'IITM', 'Prof. Janakiram', 'MLC', 'AWS', 'Cloud'])\n",
    "\n",
    "Which of the following commands will result in the output as 'upGrad'?\n",
    "\n",
    "\n",
    "rdd.collect()\n",
    "\n",
    "\n",
    "rdd1.first()    ✓ Correct\n",
    "\n",
    "rdd1.filter(lambda x: x[0] =='u')\n",
    "\n",
    "\n",
    "rdd1.take(1)    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a57bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1b93620",
   "metadata": {},
   "source": [
    "Apache Spark\n",
    "‘Fine-grained’ means that you can transform individual elements of a data set. ‘Coarse-grained’ means that you can transform an entire data set but not an individual element of it. \n",
    "Which of the following statements is true about RDDs?\n",
    "\n",
    "\n",
    "The read operation on an RDD can either be fine-grained or coarse-grained.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5ba80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "868f5f1c",
   "metadata": {},
   "source": [
    "RDD Operations\n",
    "Which of the following is not a transformation?\n",
    "\n",
    "\n",
    "sample()\n",
    "\n",
    "\n",
    "map()\n",
    "\n",
    "\n",
    "reduce()   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755dfef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fc4ccf1",
   "metadata": {},
   "source": [
    "RDD Operations\n",
    "In a given RDD, you need to find out the number of occurrences of a particular element of the RDD. Which of the following actions will you perform?\n",
    "\n",
    "\n",
    "count()\n",
    "\n",
    "\n",
    "take()\n",
    "\n",
    "\n",
    "fold()\n",
    "\n",
    "\n",
    "countByValue()   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998baab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5135568",
   "metadata": {},
   "source": [
    "## Lazy Evaluation in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efae13",
   "metadata": {},
   "source": [
    "DAG\n",
    "Consider the following DAG showing an overall execution path of a Spark application and answer the following question.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "What is the total number of transformations included in a Spark application?\n",
    "\n",
    "\n",
    "7\n",
    "\n",
    "\n",
    "6\n",
    "\n",
    "\n",
    "5   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b7a83fa",
   "metadata": {},
   "source": [
    "DAG\n",
    "Consider the following DAG showing an overall execution path of a Spark application and answer the following question.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Suppose a node on the cluster, storing one of the partitioned data of RDD5, loses all its data. Now, in order to recover the partitions of RDD5, the lineage graph of RDD5 is referred to, which is:\n",
    "\n",
    "\n",
    "RDD1→RDD2→RDD3→RDD4→RDD5​​​\n",
    "\n",
    "\n",
    "RDD1→RDD2→RDD3→RDD5​​   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc880b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f3a8425",
   "metadata": {},
   "source": [
    "DAG\n",
    "RDD Lineage (a.k.a RDD operator graph or RDD dependency graph) is a graph that consists of the parent RDDs of an RDD. It is built as a result of applying transformations to the RDD.\n",
    "join: It is a transformation for joining two paired RDDs based on common keys. If there are two RDDs of <x,y> and <x,z> types, then the resultant RDD will be in the form of <x,(y,z)>.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Let's consider this as the dependency or lineage graph for RDD R33. From the diagram above, identify the RDDs that are directly created by referring to external datasets in a single step.\n",
    "\n",
    "\n",
    "R00 and R01  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04528d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c940cf4",
   "metadata": {},
   "source": [
    "# Paired RDDs\n",
    "Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a8f7e",
   "metadata": {},
   "source": [
    "## Paired RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b0b3b",
   "metadata": {},
   "source": [
    "Paired RDDs\n",
    "Suppose you have the following RDD:\n",
    "\n",
    "rdd1 = sc.parallelize(['upGrad', 'IIITB', 'SME', 'DE', 'AWS', 'Cloud'])\n",
    "Select the code that gives the number of characters in each element of the RDD along with it.\n",
    "\n",
    "Resultant RDD: [('upGrad', 6),  ('IIITB', 5),  ('SME', 3),  ('DE', 2),  ('AWS', 3),  ('Cloud', 5)]\n",
    "\n",
    "\n",
    "rdd1.map(x, len(x))\n",
    "\n",
    "\n",
    "rdd1.map(lambda x: (x, count(x)))\n",
    "\n",
    "\n",
    "rdd1.flatMap(lambda x: (x, len(x)))\n",
    "\n",
    "\n",
    "rdd1.map(lambda x: (x, len(x))) ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be37b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154afc55",
   "metadata": {},
   "source": [
    "Paired RDDs\n",
    "Suppose you have the following RDD that represents the marks of students in different subjects out of 100:\n",
    "\n",
    "marks = sc.parallelize([('Phanendra', 'English', 98), ('Kaustubh', 'Mathematics', 99), ('Vishal', 'English', 95), ('Kaustubh', 'Science', 100), ('Phanendra', 'Science', 94), ('Vishal', 'Science', 97), ('Kaustubh', 'English', 96), ('Vishal', 'Mathematics', 98), ('Phanendra', 'Mathematics', 98)])\n",
    "Which of the following codes will result in the aggregate percentage scored by the students in the class?\n",
    "\n",
    "\n",
    "marks.groupByKey().mapValues(sum).map(lambda x: (x[0], x[1]/3)).collect()\n",
    "\n",
    "\n",
    "marks.map(lambda x: (x[0], x[2])).reduceByKey(lambda x,y: (x+y)).map(lambda x: (x[0], x[1]/3)).collect()  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa4df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa20a47e",
   "metadata": {},
   "source": [
    "Paired RDDs\n",
    "Suppose you have the following RDD that represents the marks of students in different subjects out of 100:\n",
    "\n",
    "marks = sc.parallelize([('Phanendra', 'English', 98), ('Kaustubh', 'Mathematics', 99), ('Vishal', 'English', 95), ('Kaustubh', 'Science', 100), ('Phanendra', 'Science', 94), ('Vishal', 'Science', 97), ('Kaustubh', 'English', 96), ('Vishal', 'Mathematics', 98), ('Phanendra', 'Mathematics', 98)])\n",
    "Which of the following codes will help in obtaining the highest score for each subject?\n",
    "\n",
    "\n",
    "marks.map(lambda x: (x[1], x[2])).reduceByKey(max).collect()    ✓ Correct\n",
    "\n",
    "marks.reduceByKey(max).collect()\n",
    "\n",
    "\n",
    "marks.map(lambda x: (x[1], x[2])).groupByKey().map(lambda x: (x[0], max(x[1]))).collect()    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e8f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc0cc48d",
   "metadata": {},
   "source": [
    "## Operations on Paired RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42b374",
   "metadata": {},
   "source": [
    "RDD Operations\n",
    "In a given paired RDD, if we want to find out whether a certain key is present or not which of the following commands should be used?\n",
    "\n",
    "\n",
    "collectAsMap()\n",
    "\n",
    "\n",
    "lookup()   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87798c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8c25727",
   "metadata": {},
   "source": [
    "Paired RDDs\n",
    "Which of the following statements is/are correct?\n",
    "\n",
    "\n",
    "The result of rdd1.leftOuterJoin(rdd2) will contain all those elements whose keys are present in rdd1.   ✓ Correct\n",
    "\n",
    "The result of rdd1.rightOuterJoin(rdd2) will contain all those elements whose keys are present in rdd1.\n",
    "\n",
    "\n",
    "The result of rdd1.cogroup(rdd2) will contain all those elements whose keys are present in rdd1.\n",
    "\n",
    "\n",
    "The result of rdd1.cogroup(rdd2) will contain all those elements whose keys are present in both the RDDs.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b251225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee84108",
   "metadata": {},
   "source": [
    "## Word Count Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdd8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD Operations\n",
    "Which of the following statements is/are correct?\n",
    "\n",
    "\n",
    "rdd1.map() returns the same number of elements as present in the rdd1.    ✓ Correct\n",
    "\n",
    "rdd1.flatMap() returns the same number of elements as present in the rdd1.\n",
    "\n",
    "\n",
    "The rdd1.flatmap() does not return the same number of elements as present in the rdd1.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a5695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7430e90f",
   "metadata": {},
   "source": [
    "# Spark Structured APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43c773",
   "metadata": {},
   "source": [
    "## DataFrames and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb60bde",
   "metadata": {},
   "source": [
    "RDDs vs Dataframes\n",
    "What are the benefits of using dataframes over RDDs?\n",
    "\n",
    "\n",
    "API in multiple languages\n",
    "\n",
    "\n",
    "Data type safety is offered in dataframes   ✓ Correct\n",
    "\n",
    "Dataframes optimes execution plan   ✓ Correct\n",
    "\n",
    "Dataframe API can infer the schema of the data   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc80d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a289080",
   "metadata": {},
   "source": [
    "RDDs vs Dataframes/Datasets\n",
    "A data set is given to you, it is quite large (in Gbs). The column names in the data set are not standardised yet. To call column type transformations on this kind of data which API is most suitable?\n",
    "\n",
    "\n",
    "Dataset   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435e74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "161dde92",
   "metadata": {},
   "source": [
    "Encoders\n",
    "What is the function of encoders?\n",
    "\n",
    "\n",
    "Encryption of the data so that it is safe on the cluster\n",
    "\n",
    "\n",
    "Conversion of high-level instructions to sparks internal binary format.\n",
    "\n",
    "\n",
    "Conversion of spark tabular data to JVM objects   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35500da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c37dcfff",
   "metadata": {},
   "source": [
    "Spark APIs\n",
    "GC(Garbage Collector) in is responsible for cleaning both off-heap and on-heap memory.\n",
    "Please choose whether the above statement is True or False.\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d1b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "718469b0",
   "metadata": {},
   "source": [
    "## Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db263493",
   "metadata": {},
   "source": [
    "RDDs vs Dataframes\n",
    "Which of the following Spark data structure/s leverage/s the benefits of the Catalyst optimiser?\n",
    "\n",
    "\n",
    "RDD\n",
    "\n",
    "\n",
    "Dataframe   ✓ Correct\n",
    "\n",
    "Dataset   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf0ab0f8",
   "metadata": {},
   "source": [
    "RDDs vs Dataframes\n",
    "Identify the Catalyst transformation in the situation given below:\n",
    "\"If your query only utilises a set of columns rather than the entire column list present in the data, then the Catalyst optimiser will try to fetch the relevant columns that are used in the query, to reduce storage and computation.\"\n",
    "\n",
    "\n",
    "Predicate pushdown\n",
    "\n",
    "\n",
    "Column pruning  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdd304d",
   "metadata": {},
   "source": [
    "## Getting Started with DataFrame APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06331322",
   "metadata": {},
   "source": [
    "Loading CSV\n",
    "Analysis needs to be carried out on a dataset stored in data.csv. Which of the following commands will read the data from the CSV into a dataframe?\n",
    "\n",
    "\n",
    "spark.read.load(“data.csv”,format = “csv”)   ✓ Correct\n",
    "\n",
    "spark.read.csv(“data.csv”)   ✓ Correct\n",
    "\n",
    "spark.sql(“select * from csv.data.csvdata.csv\")   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f1984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604650a6",
   "metadata": {},
   "source": [
    "InferSchema\n",
    "State whether the following statement is true or false.\n",
    "‘If inferSchema is set as true, Spark will set the data types for all the columns.’\n",
    "\n",
    "\n",
    "True  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba93bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5101cb9e",
   "metadata": {},
   "source": [
    "File Formats\n",
    "The following read command was used to read data and it was executed successfully.\n",
    "\n",
    "spark.read.load(\"filename.fileformat\")\n",
    "What should be the file format here?\n",
    "\n",
    "\n",
    "Csv\n",
    "\n",
    "\n",
    "Json\n",
    "\n",
    "\n",
    "ORC\n",
    "\n",
    "\n",
    "Parquet  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c7071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d6db239",
   "metadata": {},
   "source": [
    "## DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213d747",
   "metadata": {},
   "source": [
    "Dataframes\n",
    "Consider the following code:\n",
    "\n",
    "df2 = df.select(df.name).orderBy(df.name.desc())\n",
    "df2  is a dataframe with:\n",
    "\n",
    "\n",
    "All columns in df in ascending order\n",
    "\n",
    "\n",
    "All columns in df in descending order\n",
    "\n",
    "\n",
    "The column ‘name’ in descending order  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097bb0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33410f45",
   "metadata": {},
   "source": [
    "Dataframes\n",
    "Consider the following code:\n",
    "\n",
    "df.describe()\n",
    "What will be the output of the code?\n",
    "\n",
    "\n",
    "The statistical description of all the numerical columns in df\n",
    "\n",
    "The object df and its memory location   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1d623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c14fa925",
   "metadata": {},
   "source": [
    "Spark SQL API\n",
    "Consider the following code:\n",
    "\n",
    "df2 = df.filter(\"speed>100\")  #filters and gives a new dataframe with all speeds more than 100\n",
    "\n",
    "df2    #show the new dataframe df2\n",
    "What will be the output of the command?\n",
    "\n",
    "\n",
    "It will show the new dataframe created\n",
    "\n",
    "\n",
    "It will show that df2 is a dataframe and show its alloted memory position   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ea5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a19eadc",
   "metadata": {},
   "source": [
    "# Graded Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee2cd8",
   "metadata": {},
   "source": [
    "## Graded Questions Part - I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe996a37",
   "metadata": {},
   "source": [
    "Q1 Spark Architecture\n",
    "Feature\tElement\n",
    "1. To control the execution of Spark application\tI. Driver node\n",
    "2. To execute tasks associated with a Spark application\tII. Driver program\n",
    " \tIII. Cluster Manager\n",
    " \tIV. Executor\n",
    "\n",
    "1 - II, 2 - III\n",
    "\n",
    "\n",
    "1 - I, 2 - IV\n",
    "\n",
    "\n",
    "1 - II, 2 - I\n",
    "\n",
    "1 - II, 2 - IV   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede92b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ac798f3",
   "metadata": {},
   "source": [
    "Q2 Spark RDDs\n",
    "Select the correct statement from the following options.\n",
    "\n",
    "\n",
    "The RDDs are loaded as soon as a transformation is executed in Spark.\n",
    "\n",
    "\n",
    "The RDDs are loaded in the executor memory as soon as the parallelize() command is executed.\n",
    "\n",
    "\n",
    "The RDDs are lazily loaded into the Executor memory.   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1c0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51c95756",
   "metadata": {},
   "source": [
    "Q3 Spark SQL\n",
    "Let's say we have a variable 'hdfspath' containing a valid path to an HDFS file. What is wrong with the following snippet of code?\n",
    "\n",
    "Code Format:\n",
    "\n",
    "df = spark.read.json(hdfspath)\n",
    "spark.sql(\"select count(reviewText), asin from df group by asin\")\n",
    " \n",
    "\n",
    "\n",
    "The code is fine.\n",
    "\n",
    "\n",
    "You cannot use a 'count' type aggregation on a field containing strings (i.e., reviewText).\n",
    "\n",
    "\n",
    "Temporary table has to be created from the dataframe.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca6968c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "208b1a78",
   "metadata": {},
   "source": [
    "Q4 Spark Session\n",
    "Which of the following are the properties of a spark session?\n",
    "\n",
    "\n",
    "It provides a gateway to access all of the spark contexts functionality\n",
    "\n",
    "✓ Correct\n",
    "Feedback:\n",
    "With a spark session, there is no need to create new spark context. A Spark session combines all the different APIs.\n",
    "\n",
    "\n",
    "It can handle HIVE, SQL APIs  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb874f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de6486d9",
   "metadata": {},
   "source": [
    "Q5 Dataframe Operations\n",
    "Suppose you are given a dataframe with the following columns:\n",
    "Roll_No, Age, Weight, Height, Medical_Alleries.\n",
    "\n",
    "You are supposed to find out the number of students whose weight is above 60 kg and height is greater than 5 feet. Which of the following commands will yield the required solution correctly?\n",
    "\n",
    "\n",
    "df.filter((df['weight']>60.0) & (df['Height’]>5.0)).show() ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc524f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "209b14e2",
   "metadata": {},
   "source": [
    "Q6 Dataframe Operations\n",
    "Consider the following code:\n",
    "\n",
    "df2 = df.select(df.name).orderBy(df.name.desc())\n",
    "df2 is a dataframe with:\n",
    "\n",
    "\n",
    "All rows in df in ascending order\n",
    "\n",
    "\n",
    "The resultant dataset will contain all the columns present in df2\n",
    "\n",
    "\n",
    "The resultant dataset will contain only the 'name' column arranged in descending order.  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ff5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e72b01c0",
   "metadata": {},
   "source": [
    "## Graded Questions Part - II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955051d",
   "metadata": {},
   "source": [
    "Q7 Dataframe Operations\n",
    "Using the data frame abstraction, calculate the number of ‘Iris_setosa’ species:\n",
    "\n",
    "\n",
    "df.filter(“species” == 'Iris-setosa').count(), 50\n",
    "\n",
    "\n",
    "df.filter(df[“species”] == 'Iris-setosa').count(), 50 ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d540e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "942b5c0f",
   "metadata": {},
   "source": [
    "Q8 Dataframe Operations\n",
    "Analyse the 'Iris-versicolor' species of the flower and calculate the sum of all ‘sepal_width’ and ‘petal_length’ for this species.\n",
    "\n",
    "\n",
    "df.filter(groupBy('species').sum('sepal_width','petal_length').show()\n",
    "\n",
    "sepal_width= 135.3, petal_length= 221.5\n",
    "\n",
    "\n",
    "df.filter(df['species'] == 'Iris-versicolor').groupBy('species').add('sepal_width','petal_length').show()\n",
    "\n",
    "sepal_width= 140.5, petal_length= 200\n",
    "\n",
    "\n",
    "df.filter(df['species'] == 'Iris-versicolor').groupBy('species').sum('sepal_width','petal_length').show()\n",
    "\n",
    "sepal_width= 138.5, petal_length= 212.97  ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd617ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a38b319",
   "metadata": {},
   "source": [
    "Q9 Dataframe Operations\n",
    "Is there any ‘Iris-setosa'  species with sepal_width greater than 4.0 and sepal_width less than 5.0? If yes, find out how many?\n",
    "\n",
    "\n",
    "No\n",
    "\n",
    "\n",
    "df.filter((df['species']==\"Iris-setosa\") & (df['sepal_width']>4) | (df['sepal_width']<5)).count(), 1\n",
    "\n",
    "\n",
    "df.filter((df['species']==\"Iris-setosa\") & (df['sepal_width']>4) & (df['sepal_width']<5)).count(), 3    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da1bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be99370a",
   "metadata": {},
   "source": [
    "Q10 Dataframe Operations\n",
    "Calculate the minimum petal_width for ‘Iris-virginica’ species.\n",
    "\n",
    "\n",
    "df.filter(df['species'] == 'Iris-virginica' ).groupBy('species').min('petal_width').show(), 1.4    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ec8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e915484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d9199e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Committed successfully! https://jovian.ai/ashutoshgole18/data-engineering-module-13-introduction-to-apache-spark\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ashutoshgole18/data-engineering-module-13-introduction-to-apache-spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de41f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25380131",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+2003 (Temp/ipykernel_15508/1923595185.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ashut\\AppData\\Local\\Temp/ipykernel_15508/1923595185.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(a ,b)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+2003\n"
     ]
    }
   ],
   "source": [
    "def t ( a, b=10):\n",
    " print(a ,b) \n",
    "t(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47014784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "var = 10\n",
    "def foo():\n",
    "  var = 15\n",
    "foo()\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6837ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "  x = 'Python'\n",
    "  def bar():\n",
    "    nonlocal x\n",
    "    x = 'Java'\n",
    "  bar() \n",
    "  return x\n",
    "\n",
    "print(foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc1f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -1\n"
     ]
    }
   ],
   "source": [
    "def func(x = 1, y = 2):\n",
    "  return x + y, x - y\n",
    "\n",
    "x, y = func(y = 2, x = 1)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de0e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 -100\n"
     ]
    }
   ],
   "source": [
    "def f(x = 100, y = 100): \n",
    " return(x+y, x-y) \n",
    "x, y = f(y = 200, x = 100) \n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76a7a6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+2003 (Temp/ipykernel_15508/4063965304.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ashut\\AppData\\Local\\Temp/ipykernel_15508/4063965304.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    global var1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+2003\n"
     ]
    }
   ],
   "source": [
    "var1 = 10\n",
    "def scope_of_var():\n",
    " global var1\n",
    " var1 = var1+5 \n",
    "scope_of_var()\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2659eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [9, 41, 19, 22, 74, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e67720ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 22]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ebd9125",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+2003 (Temp/ipykernel_15508/1065393685.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ashut\\AppData\\Local\\Temp/ipykernel_15508/1065393685.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print('ok')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+2003\n"
     ]
    }
   ],
   "source": [
    "d = {'a': 1, 'b': 1, 'c': 0}\n",
    "if d['a'] > 0: \n",
    " print('ok')\n",
    "elif d['d'] > 0: \n",
    " print('ok')\n",
    "else:\n",
    " print('not ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1049d7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+2003 (Temp/ipykernel_15508/1240260631.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ashut\\AppData\\Local\\Temp/ipykernel_15508/1240260631.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print(i)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+2003\n"
     ]
    }
   ],
   "source": [
    "l=list(range(1, 0, 0))\n",
    "for i in l:\n",
    " print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93fef730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a=[3,4,9,0,1,12,5]\n",
    "b=a.sort()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2750ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15508/2875649714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;34m\"1\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "print( 1 < \"1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b933efc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15508/758230663.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"1\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "print( 1 > \"1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622dcc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print( 1 == \"1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "088ac74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print( 1 != \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ab7977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 2, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "def my_func(num):\n",
    "  data = [0]\n",
    "  for i in range(1, num+1):\n",
    "    data.append(data[i >> 1] + int(i & 1))\n",
    "  return data\n",
    "\n",
    "\n",
    "num = 6\n",
    "print(my_func(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cafbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def oddsums(n):\n",
    "  total=0\n",
    "  result=[]\n",
    "  for i in range(1,n+1):\n",
    "    odd= 2*i-1\n",
    "    total=total+odd\n",
    "    result.append(total)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a0a00f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+200B (Temp/ipykernel_15508/1524819139.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ashut\\AppData\\Local\\Temp/ipykernel_15508/1524819139.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    if list1 is list2:​\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+200B\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "if list1 is list2:​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dcb26e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "  def __init__(self, id):\n",
    "    self.id = id\n",
    "\n",
    "sam = Person(100)\n",
    "\n",
    "sam.__dict__['age'] = 49\n",
    "\n",
    "print (sam.age + len(sam.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0701f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.5\n"
     ]
    }
   ],
   "source": [
    "val1=28**0\n",
    "val2=2.5\n",
    "val3='123'\n",
    "val4=int(val3)\n",
    "print(val1+val2+val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb40583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
