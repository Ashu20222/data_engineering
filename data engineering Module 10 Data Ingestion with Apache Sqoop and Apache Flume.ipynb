{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4f4ec5",
   "metadata": {},
   "source": [
    "# Introduction to Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3c3d8",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8366a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d9b23e0",
   "metadata": {},
   "source": [
    "Data Ingestion\n",
    "Select the option that best describes data ingestion.\n",
    "\n",
    "\n",
    "It is the process of absorbing something.\n",
    "\n",
    "\n",
    "It is the process of absorbing data for immediate use or storage.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99201657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1c9be5b",
   "metadata": {},
   "source": [
    "Data Ingestion\n",
    "Why can’t big data systems ingest all types of data at once? (Note: More than one option may be correct.)\n",
    "\n",
    "\n",
    "In the big data ecosystem, different types of data have different methods of storage.    ✓ Correct\n",
    "\n",
    "Different types of data are often used for different types of analysis.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b81be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5b33e48",
   "metadata": {},
   "source": [
    "## Challenges in Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd18ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1266226f",
   "metadata": {},
   "source": [
    "Network challenges\n",
    "Which of the following could be some of the network challenges in data ingestion? (Note: More than one option may be correct.)\n",
    "\n",
    "\n",
    "Network bandwidth and speed should be satisfactory according to your needs.     ✓ Correct\n",
    "\n",
    "Network security is a major aspect that you should consider while ingesting data from unknown sources.     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00de14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa85419c",
   "metadata": {},
   "source": [
    "Challenges in Data Ingestion\n",
    "How does data being generated at high speed affect the performance of data ingestion?\n",
    "\n",
    "\n",
    "It does not affect data ingestion.\n",
    "\n",
    "\n",
    "Data has to be managed properly so that it does not impose too much load on the systems at the destination.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf037c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e724658",
   "metadata": {},
   "source": [
    "## Key Steps of Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa77e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86eb0449",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'You need to prioritise the data sources that you use for importing data according to your requirement in order to ensure optimal use of processing power, storage and time.'\n",
    "\n",
    "\n",
    "True     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef8e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c084bfd",
   "metadata": {},
   "source": [
    "Data Validation\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'For a recommender system, you should prioritise user identification information, such as user name and user ID, for data ingestion.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a287d9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d93ee7e",
   "metadata": {},
   "source": [
    "## Tools for Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bbfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3de572e",
   "metadata": {},
   "source": [
    "Tools for Data Ingestion\n",
    "Which of the following commands is used for data ingestion tasks in Hadoop?\n",
    "\n",
    "\n",
    "cd\n",
    "\n",
    "\n",
    "mv\n",
    "\n",
    "\n",
    "put    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eaef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e596e5f",
   "metadata": {},
   "source": [
    "Tools for Data Ingestion\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'Kafka enables programmers to pass messages from one point to another'\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67a01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09bfa883",
   "metadata": {},
   "source": [
    "## Types of Data and File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddfbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e54075",
   "metadata": {},
   "source": [
    "Types of Data\n",
    "Which of the following types of data has the majority in terms of the volume of production in the industry?\n",
    "\n",
    "\n",
    "Structured data\n",
    "\n",
    "\n",
    "Unstructured data   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20701550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec4fda5b",
   "metadata": {},
   "source": [
    "File Formats\n",
    "Which of the following file formats does not support block compression?\n",
    "\n",
    "\n",
    "Text/CSV files   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b5b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112b0bae",
   "metadata": {},
   "source": [
    "# Apache Sqoop - I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851530be",
   "metadata": {},
   "source": [
    "## Introduction to Sqoop and its Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e30c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82473091",
   "metadata": {},
   "source": [
    "Apache Sqoop\n",
    "State whether the following state is true or false:\n",
    "\n",
    "'Sqoop allows data transfer in only one direction, i.e., from RDBMSes to the HDFS.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2b54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fcd0604",
   "metadata": {},
   "source": [
    "Apache Sqoop\n",
    "Which of the following is/are advantages of using Sqoop?\n",
    "\n",
    "\n",
    "Cost-efficient   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c650497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e939ce",
   "metadata": {},
   "source": [
    "Apache Sqoop\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'All Sqoop jobs are map-only. They do not require the ‘reduce’ phase.'\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458e885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6bd9357",
   "metadata": {},
   "source": [
    "## Apache Sqoop Set-Up and Database Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52fc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c544eb3a",
   "metadata": {},
   "source": [
    "Apache Sqoop Set-Up and Database Set-Up\n",
    "Which of the following is the correct command to access MySQL in the EMR instance(post setting up MySQL on EMR instance)?\n",
    "\n",
    "\n",
    "mysql -u admin - p, Password - 123\n",
    "\n",
    "\n",
    "mysql -u root - p, Password - pass\n",
    "\n",
    "\n",
    "mysql -u root - p, Password - 123   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec4dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72bda619",
   "metadata": {},
   "source": [
    "Apache Sqoop Set-Up and Database Set-Up\n",
    "Which of the following commands can be used to download a file from the internet?\n",
    "\n",
    "\n",
    "put\n",
    "\n",
    "\n",
    "ls\n",
    "\n",
    "\n",
    "hadoop\n",
    "\n",
    "\n",
    "wget    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c0a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f1d2466",
   "metadata": {},
   "source": [
    "## Exporting Data - Sqoop Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0a6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e57d73b",
   "metadata": {},
   "source": [
    "Sqoop Export\n",
    "What is the function of the ‘--connect’ argument in the Sqoop export command?\n",
    "\n",
    "\n",
    "It gives the JDBC url where the MySQL table is located.    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e567149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d167fa",
   "metadata": {},
   "source": [
    "Sqoop Export\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'Before exporting data to MySQL tables, you need to ensure that the corresponding tables are already present in the RDBMS.'\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015c55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d96457dc",
   "metadata": {},
   "source": [
    "## Importing Data - Sqoop Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59167e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12b38c75",
   "metadata": {},
   "source": [
    "Sqoop Import\n",
    "What will happen if you do not use the ‘--target-dir’ argument in the Sqoop import command?\n",
    "\n",
    "\n",
    "The table will be imported to an arbitrary location in the HDFS.\n",
    "\n",
    "\n",
    "An error will be thrown, with a message saying no target directory was provided.\n",
    "\n",
    "\n",
    "The table will be imported to a folder with the same name as that of the table in the home directory of the HDFS.    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e71f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc20699b",
   "metadata": {},
   "source": [
    "Sqoop Import\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'Before running the Sqoop import command, you need to ensure that the target directory does not already exist in HDFS.'\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5ad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "902b42a3",
   "metadata": {},
   "source": [
    "## Importing Data - Importing All Tables Using Sqoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a63abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55dd73c4",
   "metadata": {},
   "source": [
    "Sqoop Import\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'All the tables to be imported from MySQL should have a primary key defined.'\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12230688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3913833b",
   "metadata": {},
   "source": [
    "## Importing Data - Handling NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963831f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65d0b535",
   "metadata": {},
   "source": [
    "Handling NULL values\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'By default, Sqoop encodes a missing value to ‘null’ in lowercase.'\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40955f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e13abe1",
   "metadata": {},
   "source": [
    "Handling NULL values\n",
    "Why is the default ‘null’ string not enough for dealing with missing values?\n",
    "\n",
    "\n",
    "The data itself may have NULL as a string/regular value instead of a missing value.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a9a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd335e3",
   "metadata": {},
   "source": [
    "## Importing Data - Handling Mappers for a Sqoop Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2d6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba059db",
   "metadata": {},
   "source": [
    "Handling Mappers\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'Increasing the number of Mappers will always lead to an increase in performance of the Sqoop job due to an increase in parallelism.'\n",
    " \n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354fa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5475e90f",
   "metadata": {},
   "source": [
    "## Importing Data - Importing in various File Formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0be021e4",
   "metadata": {},
   "source": [
    "Sqoop Import\n",
    "Which of the following commands is used for storing data as a parquet file?\n",
    "\n",
    "\n",
    "--as-pfile\n",
    "\n",
    "\n",
    "--as-parquet\n",
    "\n",
    "\n",
    "--as-parquetfile    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce7fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6796fdb3",
   "metadata": {},
   "source": [
    "Parquet Files\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'Parquet files enhance performance in big-data-related tasks by 10x in all kinds of workflows.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c4c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfcfc93b",
   "metadata": {},
   "source": [
    "## Importing Data - Compression using Sqoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c233c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f330585",
   "metadata": {},
   "source": [
    "Compression in Sqoop Import\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'Using the ‘--compress’ argument is strictly better than using the SnappyCodec for compression purposes in terms of the size of the final file generated.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bd3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a546bca8",
   "metadata": {},
   "source": [
    "## Extra Coding Questions - Sqoop - I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164583e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a060bb3a",
   "metadata": {},
   "source": [
    "Sqoop - I\n",
    "Import all the records from the ‘movies’ table and find out the number of records that are imported by the Sqoop job. Use ‘/user/root/MovieLens/movies’ as the target directory for the import.\n",
    "\n",
    "\n",
    "1,682    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f52898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4f5c85",
   "metadata": {},
   "source": [
    "Sqoop - I\n",
    "Import the records from all the tables of the MovieLens data set and find out the total number of records that are imported by the Sqoop job. Use ‘/user/root/MovieLens/All_tables’ as the warehouse directory for the import.\n",
    "\n",
    "\n",
    "1,00,000\n",
    "\n",
    "\n",
    "1,05,555    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc972c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a301c410",
   "metadata": {},
   "source": [
    "Sqoop - I\n",
    "Type the following commands in MySQL to insert some NULL records into the ‘Users’ table of the MovieLens database.\n",
    "\n",
    "mysql> insert into users values(944, null, \"M\", null, null);\n",
    "mysql> insert into users values(945, null, \"M\", null, null);\n",
    "mysql> insert into users values(946, null, \"F\", null, null);\n",
    "Import the records from the ’Users’ table and find out the number of records imported by the Sqoop job. Use ‘/user/root/MovieLens/users_NULL’ as the target directory for the import. Also, while importing, the string values where null is present are to be replaced with ‘\\\\N’ and non-string values with ‘\\\\N’.\n",
    "\n",
    "\n",
    "946      ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dbcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ea0cc7a",
   "metadata": {},
   "source": [
    "Sqoop - I\n",
    "Import the records from the 'Genres' table. Use three mappers for this task and find out the number of part files generated by the Sqoop import command. Use ‘/user/root/MovieLens/genres’ as the target directory for the import.\n",
    "\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "3     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fdd5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e27fdd45",
   "metadata": {},
   "source": [
    "Sqoop - I\n",
    "Import the records from the 'Users' table and store them as a parquet file and a sequence file. Also, find out the number of records imported by the Sqoop job for both the file formats. Use ‘/user/root/MovieLens/users_parquet’ and ‘/user/root/MovieLens/users_sequence’ as the target directories for the parquet and sequence files, respectively, for the import.\n",
    "\n",
    "\n",
    "945\n",
    "\n",
    "\n",
    "942\n",
    "\n",
    "\n",
    "946    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fc2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9d1681c",
   "metadata": {},
   "source": [
    "Sqoop - I\n",
    "Import the records from the 'Movies' table and store them in a compressed format using SnappyCodec. Also, find out the space occupied by the output files when they are compressed and when they are not. Use ‘/user/root/MovieLens/movies_compressed’ as the target directory for the import.\n",
    "\n",
    "\n",
    "69 KB, 40 KB\n",
    "\n",
    "\n",
    "65 KB, 36 KB    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521b472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd748678",
   "metadata": {},
   "source": [
    "# Apache Sqoop - II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754be753",
   "metadata": {},
   "source": [
    "## Importing Data - Importing Specific rows in Sqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc42aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a74cd28",
   "metadata": {},
   "source": [
    "Importing specific rows in Sqoop\n",
    "Let’s suppose that there is a Customer Data table in MySQL and we need to import only those records from the table which has the ID greater than 300. Which of the following ‘--where’ arguments will satisfy this requirement? Assume that the name of the ID column is ‘id’\n",
    "\n",
    "\n",
    "--where “id > 300”    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d27576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "505c8d41",
   "metadata": {},
   "source": [
    "## Importing Data - SQL Queries in Sqoop Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b7c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6699ad41",
   "metadata": {},
   "source": [
    "Sqoop Import\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'The '--split-by' parameter is not mandatory while using free-form query import (--query parameter).'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3e2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b55bc89f",
   "metadata": {},
   "source": [
    "## Importing Data - Using Incremental Import in Sqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c126350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0faab4",
   "metadata": {},
   "source": [
    "Incremental Import\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'If there is no new record in the tables in MySQL, then the incremental append command will throw an error.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499332b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1323ae8",
   "metadata": {},
   "source": [
    "Incremental Import\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'The ‘--last--modified’ parameter can be used to import all the rows that have been updated.'\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26470be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a22afc",
   "metadata": {},
   "source": [
    "## Sqoop Jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97b051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f95d5291",
   "metadata": {},
   "source": [
    "Sqoop Jobs\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'You can use the ‘--list’ option with Sqoop jobs to retrieve the list of all the tables stored in a database.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dce18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "988b88b8",
   "metadata": {},
   "source": [
    "Sqoop Jobs\n",
    "Which of the following is a valid Sqoop job argument?\n",
    "\n",
    "\n",
    "--exec\n",
    "\n",
    "\n",
    "--list\n",
    "\n",
    "\n",
    "--create\n",
    "\n",
    "\n",
    "All of the above    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f6179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff7824b",
   "metadata": {},
   "source": [
    "## Tuning Sqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f962593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca541eb",
   "metadata": {},
   "source": [
    "Tuning Sqoop\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'You need to ensure that there is no ‘\\n’ character in the password text file that you use for authenticating Sqoop commands.'\n",
    "\n",
    "\n",
    "True    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7b2d8f",
   "metadata": {},
   "source": [
    "Tuning Sqoop\n",
    "Why do we provide 'chmod 400' permissions to the password file for Sqoop commands?\n",
    "\n",
    "\n",
    "The 'chmod 400' permissions allow the user to only read the password, and other users or the group of the original user do not have permission to the file.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f5f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c952cfc5",
   "metadata": {},
   "source": [
    "## Extra Coding Questions - Sqoop - II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861502da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d595ac68",
   "metadata": {},
   "source": [
    "Sqoop - II\n",
    "From the ‘genres_movies’ table, import all the records in which the genre_id is equal to 1 and find out the number of records that are imported by the Sqoop job. Use ‘/user/root/MovieLens/genres_movies_1’ as the target directory for the import.\n",
    "\n",
    "\n",
    "255\n",
    "\n",
    "\n",
    "256\n",
    "\n",
    "\n",
    "251     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ae126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04772a04",
   "metadata": {},
   "source": [
    "Sqoop - II\n",
    "Write a Sqoop import command to import the data, which comes after the join between the ‘users’ and ‘occupations’ tables. You have to import this data so that you can determine the occupation id and name of a particular user. Also, find out the number of records imported by the Sqoop job.\n",
    " \n",
    "\n",
    "\n",
    "946\n",
    "\n",
    "\n",
    "942\n",
    "\n",
    "\n",
    "943     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfceced6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba7f1604",
   "metadata": {},
   "source": [
    "Sqoop - II\n",
    "In the ‘genres’ table present in the ‘/user/root/MovieLens/genres directory’, calculate the size of the files imported into HDFS. Now, add five more records to the MySQL table as follows:\n",
    "\n",
    "mysql>insert into movielens.genres values (19,'Cat1');\n",
    "mysql>insert into movielens.genres values (20,'Cat2');\n",
    "mysql>insert into movielens.genres values (21,'Cat3');\n",
    "mysql>insert into movielens.genres values (22,'Cat4');\n",
    "mysql>insert into movielens.genres values (23,'Cat5');\n",
    "Import the ‘genres’ table into HDFS such that only newly added records are imported. Use ‘/user/root/MovieLens/genres_new’ as the target directory for the import. Find out the approximate size of the output files generated after the Sqoop job.\n",
    "\n",
    "\n",
    "35–45 bytes    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6af2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4bf78fc",
   "metadata": {},
   "source": [
    "Sqoop - II\n",
    "Create a Sqoop job that imports all the records from the ‘genres_movies’ table in which genre_id is equal to 2. Also, ensure that the password that is passed to the job is secured in a separate text file. Use ‘/input/MovieLens/genres_movies_job’ as the target directory for the import. Execute the Sqoop job and find out the number of records that it retrieved.\n",
    "\n",
    "\n",
    "135    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262b877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d213cd8f",
   "metadata": {},
   "source": [
    "# Apache Flume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92935ed6",
   "metadata": {},
   "source": [
    "## Introduction to Apache Flume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4aef13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4745c79f",
   "metadata": {},
   "source": [
    "Data Ingestion\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'When multiple servers access the HDFS cluster simultaneously, it poses a major challenge to the process of Data Ingestion.'\n",
    "\n",
    "\n",
    "True     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b6a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee72d7a2",
   "metadata": {},
   "source": [
    "Data Ingestion\n",
    "Which of the following is not a type of unstructured data?\n",
    "\n",
    "\n",
    "Log files generated by an API\n",
    "\n",
    "\n",
    "Videos and images\n",
    "\n",
    "\n",
    "Employee data in the form of SQL tables    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef02659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ee8a9c",
   "metadata": {},
   "source": [
    "## Components of Flume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25593a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa05a034",
   "metadata": {},
   "source": [
    "Components of Flume\n",
    "Which of the following is not a function of the header of an event in Flume?\n",
    "\n",
    "\n",
    "Routing events to an appropriate channel\n",
    " \n",
    "\n",
    "Uniquely identifying an event\n",
    "\n",
    "\n",
    "None of the above   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e873ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cdc6927",
   "metadata": {},
   "source": [
    "Components of Flume\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'Sources receive data from an application by listening to its ports or file system.'\n",
    "\n",
    "\n",
    "True    ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b25d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d686ec49",
   "metadata": {},
   "source": [
    "Components of Flume\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'You are limited to the pre-existing types of components of a Flume agent.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0654e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b898ebf6",
   "metadata": {},
   "source": [
    "## Characteristics and Use Cases of Flume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67819a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a03a1642",
   "metadata": {},
   "source": [
    "Characteristics of Flume\n",
    "Which of the following is/are characteristic(s) of Flume? (Note: More than one option may be correct.)\n",
    "\n",
    "\n",
    "Automatic fault tolerance     ✓ Correct\n",
    "\n",
    "Scales by itself     ✓ Correct\n",
    "\n",
    "Extendable nature     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829e2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ef99b5",
   "metadata": {},
   "source": [
    "## Flume Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c39a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1da65b8",
   "metadata": {},
   "source": [
    "Flume Configuration Files\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'It is important to define a flow for sources to be connected to their corresponding sinks.'\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e479eb6",
   "metadata": {},
   "source": [
    "Flume Configuration Files\n",
    "Which of the following is a source property? (Note: More than one option may be correct.)\n",
    "\n",
    "\n",
    "type    ✓ Correct\n",
    "\n",
    "bind    ✓ Correct\n",
    "\n",
    "port    ✓ Correct\n",
    "\n",
    "hostname    ✓ Correc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6847d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1ebf039",
   "metadata": {},
   "source": [
    "## Flume Flows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa3e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f885be3",
   "metadata": {},
   "source": [
    "Flume Flows\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "A single set of channel, source and sink connected together can form a Flume flow.\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a675dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bd15a1",
   "metadata": {},
   "source": [
    "Flume Flows\n",
    "Which of the following is an advantage of using a tiered data collection flow?\n",
    "\n",
    "\n",
    "It reduces the load on the sink connected to the HDFS.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bde3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946cae40",
   "metadata": {},
   "source": [
    "## Tuning Flume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886d819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96833946",
   "metadata": {},
   "source": [
    "Tuning Flume\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'The batch size directly affects throughput, latency and duplication under failure.'\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2516d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3208ab68",
   "metadata": {},
   "source": [
    "Tuning Flume\n",
    "State whether the following statement is true or false:\n",
    "\n",
    "'A channel's transaction capacity must always be higher than its capacity.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3e1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95953aae",
   "metadata": {},
   "source": [
    "## Sqoop vs. Flume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97923c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3630eda",
   "metadata": {},
   "source": [
    "Sqoop vs. Flume\n",
    "State whether the following statement is true or false: \n",
    "\n",
    "'Sqoop and Flume are event-driven data ingestion tools.'\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "False    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac51f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eac9e04",
   "metadata": {},
   "source": [
    "Sqoop vs. Flume\n",
    "State whether the following statement is true or false: \n",
    "\n",
    "'Flume can scale horizontally with the help of multiple Flume agents.'\n",
    "\n",
    "\n",
    "True    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a53cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8175de5",
   "metadata": {},
   "source": [
    "## Flume Practice Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cac572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f1179aa",
   "metadata": {},
   "source": [
    "1 Apache Flume\n",
    "What are the names of the sources, channels and sinks used in the Flume agent specified in the spool.conf configuration file?\n",
    "\n",
    "\n",
    "spoolsource, sachnl, avrosink\n",
    "\n",
    "\n",
    "spoolsrc, sachnl, avrosnk    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0367f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5a8984",
   "metadata": {},
   "source": [
    "2 Apache Flume\n",
    "What is the type of source specified in the spool.conf configuration file for the Flume agent?\n",
    "\n",
    "\n",
    "spooldir    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42a851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd03193",
   "metadata": {},
   "source": [
    "3 Apache Flume\n",
    "What is the directory from where the Flume agent defined in the spool.conf configuration file imports the logs?\n",
    "\n",
    "\n",
    "/tmp/UpGradLogs     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf84a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1616e9b",
   "metadata": {},
   "source": [
    "4 Apache Flume\n",
    "What is the type of sink of the Flume agent specified in the spool.conf configuration file?\n",
    "\n",
    "\n",
    "avro      ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ed48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68cf1d82",
   "metadata": {},
   "source": [
    "5 Apache Flume\n",
    "What is the type of channel of the Flume agent specified in the spool.conf configuration file?\n",
    "\n",
    "\n",
    "avro\n",
    "\n",
    "\n",
    "file    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed290f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "107e6189",
   "metadata": {},
   "source": [
    "6 Apache Flume\n",
    "What are the names of the sources, channels and sinks used in the Flume agent specified in the hdfssink.conf configuration file?\n",
    "\n",
    "\n",
    "avrosrc, hachnl, hdfssnk     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "439e5aec",
   "metadata": {},
   "source": [
    "7 Apache Flume\n",
    "What is the port number to which the source of the Flume agent specified in the hdfssink.conf configuration file, connects to?\n",
    "\n",
    "\n",
    "12345          ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbff23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0899e1fd",
   "metadata": {},
   "source": [
    "8 Apache Flume\n",
    "What is the channel capacity of the channel specified in the hdfssink.conf configuration file for the Flume agent?\n",
    "\n",
    "\n",
    "100000    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffdc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f9ee56e",
   "metadata": {},
   "source": [
    "9 Apache Flume\n",
    "What is the type of channel of the Flume agent specified in the hdfssink.conf configuration file?\n",
    "\n",
    "\n",
    "memory    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534ce0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "140f8ff6",
   "metadata": {},
   "source": [
    "10 Apache Flume\n",
    "What is the directory where the Flume agent specified in the hdfssink.conf configuration file dumps its data?\n",
    "\n",
    "\n",
    "/test/flume/data/part-00000\n",
    "\n",
    "\n",
    "/test/flume/data    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c36b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ea85016",
   "metadata": {},
   "source": [
    "# Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d72f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73745ee0",
   "metadata": {},
   "source": [
    "1 Incremental Import\n",
    "Which of the following statements is true about the --incremental append clause in Sqoop?\n",
    "\n",
    "\n",
    "It imports only newly added rows.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa4586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63aa5dbb",
   "metadata": {},
   "source": [
    "2 Apache Sqoop\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "'All Sqoop jobs are map-only. They do not require the reduce phase.'\n",
    "\n",
    "\n",
    "True   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613edac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef1470f8",
   "metadata": {},
   "source": [
    "3 Importing Specific Rows Using Sqoop\n",
    "Does using the --where parameter of the Sqoop import command lead to significant performance issues?\n",
    "\n",
    "\n",
    "Using the --where parameter does not lead to a significant performance loss, as Sqoop directly processes the data imported without any latency issues.\n",
    "\n",
    "\n",
    "Using the --where parameter always leads to a significant performance loss, as it imposes a significant burden on the database server.\n",
    "\n",
    "\n",
    "If complex function calls are specified, then the --where parameter has a significant impact on performance.    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1354f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4eb6be8",
   "metadata": {},
   "source": [
    "4 Flume Configuration Files\n",
    "A Flume agent can be configured by writing a Flume configuration file. Which of the following is a vital component of a configuration file?\n",
    "\n",
    "\n",
    "Source\n",
    "\n",
    "\n",
    "Channel\n",
    "\n",
    "\n",
    "Sink\n",
    "\n",
    "\n",
    "All of the above   ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ef41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45e68ac5",
   "metadata": {},
   "source": [
    "5 Flume Configuration Files\n",
    "Consider the following snippet from a Flume configuration file:\n",
    "\n",
    "agent1.sources = src1\n",
    "agent1.channels = ch1 ch2 ch3\n",
    "agent1.sources.src1.channels = ch1 ch2\n",
    "agent1.sources.src1.selector.type = replicating\n",
    "agent1.sources.src1.type = spooldir \n",
    "agent1.sources.src1.spoolDir = /tmp/UpGradLogs\n",
    "Which of the following statements is true about this configuration?\n",
    "\n",
    "\n",
    "All events are written in either of the two channels: ch1 and ch2.\n",
    "\n",
    "The type of the source is spooldir.    ✓ Correct\n",
    "\n",
    "All events are written in both the channels: ch1 and ch2.     ✓ Correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde9712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f69b97",
   "metadata": {},
   "source": [
    "6 Apache Flume\n",
    "State whether the following statement is true or false.\n",
    "\n",
    "'A source can be connected to multiple channels and a sink can be connected to only one channel.'\n",
    "\n",
    "\n",
    "True     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f6858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "175aec9c",
   "metadata": {},
   "source": [
    "7 Key Steps of Data Ingestion\n",
    "Assume that you have to build a recommender system for YouTube videos. Which of the following types of data will you take into consideration for data ingestion for this particular use case?\n",
    "\n",
    "\n",
    "User watch history    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7ceb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43822d24",
   "metadata": {},
   "source": [
    "8 Challenges of Data Ingestion\n",
    "Which of the following are challenges in the process of data ingestion? \n",
    "\n",
    "Note: Multiple options may be correct.\n",
    "\n",
    "\n",
    "Multiple different types of data and their sources     ✓ Correct\n",
    "\n",
    "Processing power for ingesting data     ✓ Correct\n",
    "\n",
    "Processing power for performing analysis on data\n",
    "\n",
    "\n",
    "Network challenges     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacaa6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a6469c",
   "metadata": {},
   "source": [
    "9 Types of Data\n",
    "Classify the following as structured, unstructured and semi-structured:\n",
    "\n",
    "i. YouTube videos\n",
    "\n",
    "ii. Aadhaar card data of people living in a village\n",
    "\n",
    "iii. Weather data set stored in a JSON file\n",
    "\n",
    "\n",
    "i - Structured, ii - Unstructured, iii - Semi-structured\n",
    " \n",
    "\n",
    "ii - Structured, iii - Unstructured, i - Semi-structured\n",
    "\n",
    "\n",
    "ii - Structured, i - Unstructured, iii - Semi-structured     ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e77732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b9778ac",
   "metadata": {},
   "source": [
    "10 Tools for Data Ingestion\n",
    "Suppose you are a part of a company that has recently opened a new branch and you have been tasked with the responsibility of ingesting critical data from the company’s central repository, which is composed of SQL tables. Which of the following tools will you use for this purpose?\n",
    "\n",
    "\n",
    "Sqoop    ✓ Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e93ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27bc902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"ashutoshgole18/data-engineering-module-10-data-ingestion-with-apache-sqoop-and-apache-flume\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/ashutoshgole18/data-engineering-module-10-data-ingestion-with-apache-sqoop-and-apache-flume\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ashutoshgole18/data-engineering-module-10-data-ingestion-with-apache-sqoop-and-apache-flume'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3b040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
